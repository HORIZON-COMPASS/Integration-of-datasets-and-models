{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of Contents\n",
        "1. [Context](#Context)\n",
        "2. [Step by Step Guide](#Step-by-Step-Guide)\n",
        "\n",
        "    2.1. [Obtaining and loading data](#Step-1-Obtaining-and-loading-data)\n",
        "\n",
        "    2.2. [ERA5 and ERA5-Land processing](#step-2-era5-and-era5-land-data-processing)\n",
        "\n",
        "    2.3. [EMO-1 data processing](#step-3-emo-1-data-processing)\n",
        "\n",
        "    2.4. [Data merging](#step-4-data-merging)\n",
        "\n",
        "    2.5. [Processing of combined ERA5-Land for BASD](#step-5-processing-of-combined-era5-land-for-bias-adjustment)\n",
        "\n",
        "    2.6. [Processing of combined EMO-1](#step-6-processing-of-combined-emo-1-data)\n",
        "\n",
        "    2.7. [Processing of combined EMO-1 for BASD](#step-7-processing-of-combined-emo-1-data-for-bias-adjustment)\n",
        "\n",
        "    2.8. [Bias-adjustment and statistical downscaling](#step-8-bias-adjustment-and-statistical-downscaling)\n",
        "\n",
        "    2.9. [Convert Bias-adjusted and downscaled ERA5 to EMO-1 format](#step-9-convert-bias-adjusted)\n",
        "\n",
        "    2.10. [Final post-processing of ERA-5 files for consistency with EMO-1 data](#step-10-final-post-processing)\n",
        "    \n",
        "3. [Possible fixes to common issues](#possible-fixes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Context\n",
        "This notebook will provide a step-by-step guide to perform bias adjustment and statistical downscaling with ISIMIP3BASD, ERA-5 Land, and EMO-1. Each step accompanies the code, explanation along with the necessary libraries required to run the code. To run the code, it is recommended to have Python version **3.11.6** installed along with the **Jupyter** Notebook extension.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step by Step Guide\n",
        "<a id=\"Step-by-Step-Guide\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow for merged all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"width:100%; display:block;\">\n",
        "    <div style=\"width:50%;\"><img src=\"https://naturalhazards.eu/workflow.jpg\"></div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow for yearly data\n",
        "\n",
        "<div style=\"width:100%; display:block;\">\n",
        "    <div style=\"width:50%;\"><img src=\"https://naturalhazards.eu/workflow2.jpg\"></div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installation\n",
        "\n",
        "### Environment setup\n",
        "\n",
        "The CLIMB workflow uses two separate environments:\n",
        "\n",
        "- a **core environment** for Steps 1–10 (`compass-env`),  \n",
        "- a **LISVAP environment** for Step 11 (`lisvap-env`).\n",
        "\n",
        "Both environments are defined in the repository and can be created with Conda.  \n",
        "For users who prefer `pip` and virtual environments, a `requirements.txt` file is also provided.\n",
        "\n",
        "---\n",
        "\n",
        "### Core environment for Steps 1–10 (`compass-env`)\n",
        "\n",
        "The main environment used for downloading, processing and bias-adjusting climate data  \n",
        "(Steps 1–10) is defined in `environment.yml`.\n",
        "\n",
        "This environment includes, among others:\n",
        "\n",
        "- core scientific stack: `numpy`, `pandas`, `xarray`, `scipy`, `statsmodels`, `scikit-learn`, `dask`,  \n",
        "- NetCDF/GRIB stack: `netcdf4`, `h5netcdf`, `h5py`, `eccodes`, `cfgrib`, `cftime`, `cf-units`, `iris`,  \n",
        "- geospatial libraries: `gdal`, `geopandas`, `rasterio`, `rioxarray`, `pyogrio`, `shapely`, `pyproj`, `cartopy`,  \n",
        "- visualisation and apps: `matplotlib`, `seaborn`, `plotly`, `folium`, `streamlit`, `jupyterlab`, `ipywidgets`,  \n",
        "- climate data tools: `cdsapi`, `cads-api-client`, `climate_indices`, and `cdo` (Python wrapper).\n",
        "\n",
        "#### Create and activate the environment\n",
        "\n",
        "```bash\n",
        "# From the environment folder of the repository\n",
        "conda env create -f environment.yml\n",
        "conda activate compass-env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After activation, you should be able to:\n",
        "-\trun the Jupyter notebooks (including the step-by-step guide),\n",
        "-\texecute the bash scripts for Steps 1–10,\n",
        "-\tuse cdo, nco, cfgrib, eccodes and the geospatial stack directly from this environment.\n",
        "\n",
        "If you need to update the environment after changes to environment.yml, use:\n",
        "\n",
        "```bash\n",
        "conda env update -f environment.yml --prune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### LISVAP environment for Step 11 (lisvap-env)\n",
        "\n",
        "Step 11 (PET calculation) relies on the external LISVAP tool and PCRaster.<br/>\n",
        "To keep the core workflow lightweight and to avoid conflicts, we recommend a separate environment for LISVAP, defined in environment-lisvap.yml.\n",
        "\n",
        "This environment includes:\n",
        "-\tpython=3.10,\n",
        "-\tpcraster 4.4.*,\n",
        "-\tnumpy, pandas, xarray, netcdf4, eccodes, cfgrib, cftime,\n",
        "-\tand via pip: lisflood-lisvap==1.3.0, cdsapi, cads-api-client, climate_indices.\n",
        "\n",
        "Create and activate the LISVAP environment\n",
        "```bash\n",
        "# From the environment folder of the repository\n",
        "conda env create -f environment-lisvap.yml\n",
        "conda activate lisvap-env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once activated, you can follow the instructions in step_11/readme.txt to:\n",
        "-\tadapt the config.xml file,\n",
        "-\tpoint LISVAP to the output of Step 10 and to the basemap/ directory,\n",
        "-\trun LISVAP to generate PET products.\n",
        "\n",
        "**Note: You do not need lisvap-env to run Steps 1–10.<br/>**\n",
        "This environment is only required if you want to reproduce Step 11 (LISVAP-based PET).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Alternative installation with pip (optional)\n",
        "\n",
        "For users who prefer not to use Conda, we also provide a requirements.txt file that approximates the compass-env environment.\n",
        "\n",
        "Important: Some packages (e.g. gdal, eccodes, cfgrib, pcraster) are much easier to install and manage via Conda. The pip-based installation is therefore recommended only for experienced users who can resolve system-level dependencies on their own.\n",
        "\n",
        "Example: create a virtual environment and install with pip\n",
        "\n",
        "```bash \n",
        "# Create and activate a virtual environment (Linux/macOS)\n",
        "python -m venv climb-env\n",
        "source climb-env/bin/activate\n",
        "\n",
        "# or on Windows (PowerShell)\n",
        "python -m venv climb-env\n",
        "climb-env\\Scripts\\Activate.ps1\n",
        "\n",
        "# Upgrade pip and install dependencies\n",
        "pip install --upgrade pip\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This should provide a working environment for running the notebooks and most parts of the workflow.<br/>\n",
        "However, for LISVAP (Step 11) and for more complex geospatial operations, we still recommend using the Conda-based environments described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1. Obtaining and loading data\n",
        "<a id=\"Step-1-Obtaining-and-loading-data\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to download ERA-5 Land data from Copernicus Marine API. To access ERA-5 Land data via the Copernicus Marine API, you'll need to access it through the Copernicus Climate Data Store (CDS) API.\n",
        "\n",
        "1. Go to the [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/#!/home).\n",
        "\n",
        "2. Create an account by registering if you haven't done so already.\n",
        "\n",
        "3. Install the cdsapi\n",
        "\n",
        "4. After registering, you'll receive an API key. This key needs to be saved in a .cdsapirc file in your home directory (~/.cdsapirc). More details on [https://cds.climate.copernicus.eu/how-to-api]\n",
        "\n",
        "5. Replace your_username and your_api_key with your actual username and API key from your CDS account.\n",
        "\n",
        "6. Accept the \"Terms of use\" on the end of the manage licenses page: [https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land?tab=download#manage-licences]\n",
        "\n",
        "The cdsapi is a Python package that allows you to download data programmatically.\n",
        "\n",
        "Install the cdsapi by running the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install cdsapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **.cdsapirc** file should look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "url: https://cds.climate.copernicus.eu/api/v2\n",
        "key: your_username:your_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now use the CDS API to download the ERA-5 Land data using the below code. You need to replace the relative path in the code with the path of your local directory where you would like to save the downloaded files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = 'YOUR-LOCATION/CLIMB'  # Keep location of Python to CLIMB folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import cdsapi\n",
        "import os\n",
        "\n",
        "c = cdsapi.Client()\n",
        "\n",
        "years = range(1950, 2022)\n",
        "months = range(1, 12)\n",
        "\n",
        "for y in years:\n",
        "    for m in months:\n",
        "        era5land_file = os.path.join(output_dir, 'step_1', 'ERA5Land_' + str(y) + '_' + str(m) + '.grib')\n",
        "        if os.path.isfile(era5land_file):\n",
        "            print('Already downloaded:', era5land_file)\n",
        "        else:\n",
        "            c.retrieve(\n",
        "                'reanalysis-era5-land',\n",
        "                {\n",
        "                    'variable': [\n",
        "                        '10m_u_component_of_wind', '10m_v_component_of_wind', '2m_dewpoint_temperature',\n",
        "                        '2m_temperature', 'surface_solar_radiation_downwards', 'total_precipitation'\n",
        "                    ],\n",
        "                    'year': str(y),\n",
        "                    'month': str(m),\n",
        "                    'day': [\n",
        "                        '01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
        "                        '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
        "                        '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
        "                        '28', '29', '30', '31',\n",
        "                    ],\n",
        "                    'time': [\n",
        "                        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
        "                        '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
        "                        '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
        "                        '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
        "                    ],\n",
        "                    'area': [\n",
        "                        55, 13, 48, 26,  # Poland and catchments of Polish rivers\n",
        "                    ],\n",
        "                    'format': 'grib',\n",
        "                },\n",
        "                era5land_file)\n",
        "\n",
        "        era5_file = os.path.join(output_dir, 'step_1', 'ERA5_' + str(y) + '_' + str(m) + '.grib')\n",
        "        if os.path.isfile(era5_file):\n",
        "            print('Already downloaded:', era5_file)\n",
        "        else:\n",
        "            c.retrieve(\n",
        "                'reanalysis-era5-single-levels',\n",
        "                {\n",
        "                    'product_type': 'reanalysis',\n",
        "                    'variable': [\n",
        "                        '10m_u_component_of_wind', '10m_v_component_of_wind', '2m_dewpoint_temperature',\n",
        "                        '2m_temperature', 'surface_solar_radiation_downwards', 'total_precipitation'\n",
        "                    ],\n",
        "                    'year': str(y),\n",
        "                    'month': str(m),\n",
        "                    'day': [    \n",
        "                        '01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
        "                        '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
        "                        '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
        "                        '28', '29', '30', '31',\n",
        "                    ],  \n",
        "                    'time': [   \n",
        "                        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00',\n",
        "                        '06:00', '07:00', '08:00', '09:00', '10:00', '11:00',\n",
        "                        '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
        "                        '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
        "                    ],  \n",
        "                    'area': [   \n",
        "                        55, 13, 48, 26,\n",
        "                    ],  \n",
        "                    'format': 'grib',\n",
        "                },\n",
        "                era5_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table: Description of the ERA5 and ERA5-Land variables.**\n",
        "\n",
        "| Short Name | Full Name                                   | Description                                                                                           |\n",
        "|------------|---------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
        "| `u10`      | 10m U Component of Wind                     | The eastward (u) component of the wind at 10 meters above ground level.                             |\n",
        "| `v10`      | 10m V Component of Wind                     | The northward (v) component of the wind at 10 meters above ground level.                            |\n",
        "| `dewpoint_temperature` | 2m Dewpoint Temperature              | The dew point temperature at 2 meters above ground level.                                            |\n",
        "| `t2m`      | 2m Temperature                              | The air temperature at 2 meters above ground level.                                                  |\n",
        "| `ssrd`    | Surface Solar Radiation Downwards          | The amount of solar radiation reaching the Earth's surface.                                          |\n",
        "| `tp`       | Total Precipitation                        | The total amount of precipitation (rain, snow, etc.) over a given period.                           |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the data has finished downloading, it is now ready for processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2. ERA5 and ERA5-Land data processing\n",
        "<a id=\"Step-2-ERA5processing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The script below will automate the necessary processing of downloaded ERA5-Land climate data for each year from 1950 to 2023. It converts monthly GRIB files into daily NetCDF files, calculates various climate variables (temperature, precipitation, solar radiation, wind speed, relative humidity), and performs necessary adjustments and cleanup. \n",
        "\n",
        "**NOTE:**: You must have the following libraries installed:\n",
        "1. `cdo` either by sudo apt-get install cdo or follow the official documentation.\n",
        "2. Make sure the bash is updated by use of sudo apt update if you are using WSL2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "for y in {1950..2023}\n",
        "do\n",
        "    year=\"$y\"\n",
        "    year_n=\"$((y+1))\"\n",
        "    i1=\"${output_dir}/step_1/ERA5Land_${year}_1.grib\"\n",
        "    i2=\"${output_dir}/step_1/ERA5Land_${year}_2.grib\"\n",
        "    i3=\"${output_dir}/step_1/ERA5Land_${year}_3.grib\"\n",
        "    i4=\"${output_dir}/step_1/ERA5Land_${year}_4.grib\"\n",
        "    i5=\"${output_dir}/step_1/ERA5Land_${year}_5.grib\"\n",
        "    i6=\"${output_dir}/step_1/ERA5Land_${year}_6.grib\"\n",
        "    i7=\"${output_dir}/step_1/ERA5Land_${year}_7.grib\"\n",
        "    i8=\"${output_dir}/step_1/ERA5Land_${year}_8.grib\"\n",
        "    i9=\"${output_dir}/step_1/ERA5Land_${year}_9.grib\"\n",
        "    i10=\"${output_dir}/step_1/ERA5Land_${year}_10.grib\"\n",
        "    i11=\"${output_dir}/step_1/ERA5Land_${year}_11.grib\"\n",
        "    i12=\"${output_dir}/step_1/ERA5Land_${year}_12.grib\"\n",
        "    i13=\"${output_dir}/step_1/ERA5Land_${year_n}_1.grib\"\n",
        "    o_tas=\"${output_dir}/step_2/ERA5_land_daily/tas_ERA5_${year}.nc\"\n",
        "    o_tasmin=\"${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_${year}.nc\"\n",
        "    o_tasmax=\"${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_${year}.nc\"\n",
        "    o_t_dew=\"${output_dir}/step_2/ERA5_land_daily/dew_ERA5_${year}.nc\"\n",
        "    o_sfcWind=\"${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_${year}.nc\"\n",
        "    o_rsds=\"${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_${year}.nc\"\n",
        "    o_pr=\"${output_dir}/step_2/ERA5_land_daily/pr_ERA5_${year}.nc\"\n",
        "    o_rsds_t=\"${output_dir}/step_2/ERA5_land_daily/rsds_t_ERA5_${year}.nc\"\n",
        "    o_pr_t=\"${output_dir}/step_2/ERA5_land_daily/pr_t_ERA5_${year}.nc\"\n",
        "    o_hurs=\"${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_${year}.nc\"\n",
        "\n",
        "    cdo -f nc -daymean -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tas\n",
        "    cdo -f nc -daymin -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tasmin\n",
        "    cdo -f nc -daymax -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tasmax\n",
        "    cdo -f nc expr,rsds=\"var169/86400\" -selhour,0 -selname,var169 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $i13 $o_rsds_t\n",
        "    cdo -f nc expr,pr=\"var228/86.4\" -selhour,0 -selname,var228 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $i13 $o_pr_t\n",
        "    cdo selyear,$y -shifttime,-1days $o_rsds_t $o_rsds\n",
        "    cdo selyear,$y -shifttime,-1days $o_pr_t $o_pr\n",
        "    cdo -f nc -daymean -selname,var168,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_t_dew\n",
        "    cdo expr,hurs=\"(10 ^ (7.5 * (var168-273.15) / (237.3+(var168-273.15)))) / (10 ^ (7.5 * (var167-273.15) / (237.3+(var167-273.15)))) * 100\" $o_t_dew $o_hurs\n",
        "    cdo -f nc expr,sfcWind=\"sqrt(var165*var165+var166*var166)\" -daymean -selname,var165,var166 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_sfcWind\n",
        "    rm $o_rsds_t\n",
        "    rm $o_pr_t\n",
        "    rm $o_t_dew\n",
        "\n",
        "done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, in the similar way, run the script below will automate the necessary processing of downloaded ERA5 climate data for each year from 1990 to 2022. It converts monthly GRIB files into daily NetCDF files, calculates various climate variables (temperature, precipitation, solar radiation, wind speed, relative humidity), and performs necessary adjustments and cleanup. Similarly, replace {your path} with your local directory path.\n",
        "\n",
        "This data will be helpful to gap-fill the ERA5-Land data in later stages of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "for y in {1950..2023}\n",
        "do\n",
        "    year=\"$y\"\n",
        "    i1=\"${output_dir}/step_1/ERA5_${year}_1.grib\"\n",
        "    i2=\"${output_dir}/step_1/ERA5_${year}_2.grib\"\n",
        "    i3=\"${output_dir}/step_1/ERA5_${year}_3.grib\"\n",
        "    i4=\"${output_dir}/step_1/ERA5_${year}_4.grib\"\n",
        "    i5=\"${output_dir}/step_1/ERA5_${year}_5.grib\"\n",
        "    i6=\"${output_dir}/step_1/ERA5_${year}_6.grib\"\n",
        "    i7=\"${output_dir}/step_1/ERA5_${year}_7.grib\"\n",
        "    i8=\"${output_dir}/step_1/ERA5_${year}_8.grib\"\n",
        "    i9=\"${output_dir}/step_1/ERA5_${year}_9.grib\"\n",
        "    i10=\"${output_dir}/step_1/ERA5_${year}_10.grib\"\n",
        "    i11=\"${output_dir}/step_1/ERA5_${year}_11.grib\"\n",
        "    i12=\"${output_dir}/step_1/ERA5_${year}_12.grib\"\n",
        "\n",
        "    o_t_dew=\"${output_dir}/step_2/ERA5_for_gapfill/dew_ERA5_${year}.nc\"\n",
        "    o_tas=\"${output_dir}/step_2/ERA5_for_gapfill/tas_ERA5_${year}.nc\"\n",
        "    o_tasmin=\"${output_dir}/step_2/ERA5_for_gapfill/tasmin_ERA5_${year}.nc\"\n",
        "    o_tasmax=\"${output_dir}/step_2/ERA5_for_gapfill/tasmax_ERA5_${year}.nc\"\n",
        "    o_sfcWind=\"${output_dir}/step_2/ERA5_for_gapfill/sfcWind_ERA5_${year}.nc\"\n",
        "    o_rsds=\"${output_dir}/step_2/ERA5_for_gapfill/rsds_ERA5_${year}.nc\"\n",
        "    o_pr=\"${output_dir}/step_2/ERA5_for_gapfill/pr_ERA5_${year}.nc\"\n",
        "    o_hurs=\"${output_dir}/step_2/ERA5_for_gapfill/hurs_ERA5_${year}.nc\"\n",
        "\n",
        "\t# temperature\n",
        "    cdo -f nc expr,dpt=\"var168\" -daymean -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tas\n",
        "    cdo -f nc expr,tx=\"var167\" -daymin -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tasmin\n",
        "    cdo -f nc expr,tn=\"var167\" -daymax -selname,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_tasmax\n",
        "\t# accumulations in ERA5 are hourly, different from ERA5-land    \n",
        "    cdo -f nc expr,rg=\"var169*24/86400\" -daymean -selname,var169 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_rsds\n",
        "    cdo -f nc expr,pr=\"var228*24/86.4\" -daymean -selname,var228 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_pr\n",
        "\t# convert to relative humidity    \n",
        "    cdo -f nc -daymean -selname,var168,var167 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_t_dew\n",
        "    cdo expr,hurs=\"(10 ^ (7.5 * (var168-273.15) / (237.3+(var168-273.15)))) / (10 ^ (7.5 * (var167-273.15) / (237.3+(var167-273.15)))) * 100\" $o_t_dew $o_hurs\n",
        "    cdo -f nc expr,ws=\"sqrt(var165*var165+var166*var166)\" -daymean -selname,var165,var166 -mergetime $i1 $i2 $i3 $i4 $i5 $i6 $i7 $i8 $i9 $i10 $i11 $i12 $o_sfcWind\n",
        "\t# remove temp files\n",
        "    rm $o_t_dew\n",
        "done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3. EMO-1 data processing\n",
        "<a id=\"Step-3-EMO1processing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The European Meteorological Observations (EMO) dataset is a high-resolution, gridded meteorological dataset for Europe, offering both sub-daily and daily data across multiple variables. Based on historical and real-time observations, EMO is a product of the Copernicus Emergency Management Service. The dataset includes daily totals for precipitation, minimum and maximum temperatures, wind speed, solar radiation, and water vapor pressure. Additionally, EMO provides 6-hourly data for precipitation and mean temperature. EMO-1 offers grids with a spatial resolution of 1arcminx1arcmin (approximately 1.5 km), covering the period from 1990 to 2022.\n",
        "\n",
        "To use the EMO-1 dataset for bias adjustment and downscaling with ISIMIP3BASD, it needs to be processed in the similar way as ERA5 data. First, the variables need to be processed using the script below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download EMO-1 Data\n",
        "Script use this url https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/CEMS-EFAS/meteorological_forcings/EMO-1arcmin/ for downloading and saving all files in separate folders in step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Retrieve the list of NetCDF files available in a given EMO-1 subdirectory\n",
        "def get_file_list(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    # Find all links leading to .nc files\n",
        "    file_links = [link.get('href') for link in soup.find_all('a') if link.get('href').endswith('.nc')]\n",
        "    return file_links\n",
        "\n",
        "# Download a single NetCDF file from EMO-1 and save it locally\n",
        "def download_file(file_url, save_directory):\n",
        "    response = requests.get(file_url)\n",
        "    filename = os.path.join(save_directory, file_url.split('/')[-1])\n",
        "    \n",
        "    with open(filename, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"Downloaded: {file_url}\")\n",
        "\n",
        "# Iterate over the requested EMO-1 variable folders and download all files\n",
        "def download_files_from_specific_folders(base_url, save_directory, folders):\n",
        "    for folder in folders:\n",
        "        full_folder_url = f\"{base_url}/{folder}\"\n",
        "        print(f\"Processing folder: {full_folder_url}\")\n",
        "        \n",
        "        # Create the subdirectory if it does not exist\n",
        "        folder_save_directory = os.path.join(save_directory, folder)\n",
        "        if not os.path.exists(folder_save_directory):\n",
        "            os.makedirs(folder_save_directory)\n",
        "        \n",
        "        file_list = get_file_list(full_folder_url)\n",
        "        \n",
        "        for file_link in file_list:\n",
        "            full_file_url = f\"{full_folder_url}/{file_link}\"\n",
        "            download_file(full_file_url, folder_save_directory)\n",
        "\n",
        "# Usage\n",
        "base_url = 'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/CEMS-EFAS/meteorological_forcings/EMO-1arcmin'\n",
        "save_directory = os.path.join(output_dir, \"step_3\", \"emo_data\")\n",
        "# EMO-1 variable folders to download (can be adapted by the user)\n",
        "folders = ['pr', 'rg', 'tn', 'tx', 'ws']  # List of folders to iterate through\n",
        "\n",
        "# Create the main EMO-1 download directory if it does not exist\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "download_files_from_specific_folders(base_url, save_directory, folders)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cut EMO-1 to same dimensions as ERA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "#\n",
        "# The script subsets the original EMO-1 files downloaded from\n",
        "# https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/\n",
        "# to a fixed latitude/longitude window (here: Poland). It iterates over all\n",
        "# variable subfolders and writes the cropped files to a separate directory\n",
        "# to avoid permission issues when overwriting .nc files in-place.\n",
        "#\n",
        "\n",
        "input_dir = os.path.join(output_dir, \"step_3\", \"emo_data\")\n",
        "output_subdir = os.path.join(output_dir, \"step_3\", \"emo_data\", \"cutted_emo\")\n",
        "\n",
        "# Geographic bounds for the Poland domain (can be adjusted by the user)\n",
        "lon_min, lon_max = 12.950000, 26.050000  #For Poland\n",
        "lat_min, lat_max = 47.950000, 55.050000  #For Poland\n",
        "\n",
        "# Size of the chunk along the first non-lat/lon dimension (e.g. time)\n",
        "chunk_size = 100\n",
        "\n",
        "# Copy a variable in chunks, subsetting along lat/lon indices to reduce memory usage\n",
        "def process_variable_in_chunks(src_var, dst_var, lon_indices, lat_indices):\n",
        "    full_shape = src_var.shape\n",
        "    dim_names = src_var.dimensions\n",
        "    lat_dim = dim_names.index('lat') if 'lat' in dim_names else None\n",
        "    lon_dim = dim_names.index('lon') if 'lon' in dim_names else None\n",
        "    src_slices = [slice(None)] * len(full_shape)\n",
        "    dst_slices = [slice(None)] * len(full_shape)\n",
        "    \n",
        "    if lat_dim is not None:\n",
        "        src_slices[lat_dim] = lat_indices\n",
        "        dst_slices[lat_dim] = slice(None)\n",
        "    if lon_dim is not None:\n",
        "        src_slices[lon_dim] = lon_indices\n",
        "        dst_slices[lon_dim] = slice(None)\n",
        "    if lat_dim is None and lon_dim is None:\n",
        "        dst_var[:] = src_var[:]\n",
        "        return\n",
        "\n",
        "    chunk_dims = [i for i, dim in enumerate(dim_names) if dim not in ['lat', 'lon']]\n",
        "    chunk_dim = chunk_dims[0] if chunk_dims else 0\n",
        "    \n",
        "    for start in range(0, full_shape[chunk_dim], chunk_size):\n",
        "        end = min(start + chunk_size, full_shape[chunk_dim])\n",
        "        src_chunk_slices = list(src_slices)\n",
        "        dst_chunk_slices = list(dst_slices)\n",
        "        src_chunk_slices[chunk_dim] = slice(start, end)\n",
        "        dst_chunk_slices[chunk_dim] = slice(start, end)\n",
        "        \n",
        "        chunk_data = src_var[tuple(src_chunk_slices)]\n",
        "        dst_var[tuple(dst_chunk_slices)] = chunk_data\n",
        "\n",
        "# Open an EMO-1 file, subset it to the Poland domain, and write the result\n",
        "def cut_file_for_poland(input_file_path, output_file_dir):\n",
        "    try:\n",
        "        with Dataset(input_file_path, 'r') as src:\n",
        "            lon = src.variables['lon'][:]\n",
        "            lat = src.variables['lat'][:]\n",
        "            lon_indices = np.where((lon >= lon_min) & (lon <= lon_max))[0]\n",
        "            lat_indices = np.where((lat >= lat_min) & (lat <= lat_max))[0]\n",
        "            \n",
        "            output_file_name = os.path.basename(input_file_path).replace('.nc', '_poland.nc')\n",
        "            output_file_path = os.path.join(output_file_dir, output_file_name)\n",
        "            \n",
        "            with Dataset(output_file_path, 'w') as dst:\n",
        "                dst.setncatts({a: src.getncattr(a) for a in src.ncattrs()})\n",
        "                for name, dimension in src.dimensions.items():\n",
        "                    if name == 'lon':\n",
        "                        dst.createDimension(name, len(lon_indices))\n",
        "                    elif name == 'lat':\n",
        "                        dst.createDimension(name, len(lat_indices))\n",
        "                    else:\n",
        "                        dst.createDimension(name, (len(dimension) if not dimension.isunlimited() else None))\n",
        "                \n",
        "                for name, variable in src.variables.items():\n",
        "                    if name in ['lon', 'lat']:\n",
        "                        x = dst.createVariable(name, variable.datatype, (name,))\n",
        "                    else:\n",
        "                        x = dst.createVariable(name, variable.datatype, variable.dimensions)\n",
        "                    \n",
        "                    dst[name].setncatts({a: variable.getncattr(a) for a in variable.ncattrs()})\n",
        "                    \n",
        "                    if name == 'lon':\n",
        "                        dst[name][:] = lon[lon_indices]\n",
        "                    elif name == 'lat':\n",
        "                        dst[name][:] = lat[lat_indices]\n",
        "                    else:\n",
        "                        process_variable_in_chunks(src[name], dst[name], lon_indices, lat_indices)\n",
        "\n",
        "        print(f\"Successfully created cut file: {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process file {input_file_path}. Error: {e}\")\n",
        "        raise\n",
        "\n",
        "# Process all NetCDF files in a given EMO-1 variable folder\n",
        "def process_folder(folder_path):\n",
        "    try:\n",
        "        output_subfolder = os.path.join(output_subdir, f\"Cutted_{os.path.basename(folder_path)}\")\n",
        "        if not os.path.exists(output_subfolder):\n",
        "            os.makedirs(output_subfolder)\n",
        "        \n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(\".nc\"):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                \n",
        "                print(f\"Processing file: {file_path}\")\n",
        "                cut_file_for_poland(file_path, output_subfolder)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing folder {folder_path}. Error: {e}\")\n",
        "\n",
        "# Main driver: loop over all selected EMO-1 variable folders for Poland\n",
        "def main():\n",
        "    try:\n",
        "        if not os.path.exists(output_subdir):\n",
        "            os.makedirs(output_subdir)\n",
        "        \n",
        "        subfolders = ['pd', 'pr', 'ws', 'rg', 'tn', 'tx']\n",
        "        \n",
        "        for subfolder in subfolders:\n",
        "            folder_path = os.path.join(input_dir, subfolder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                print(f\"Processing folder: {folder_path}\")\n",
        "                process_folder(folder_path)\n",
        "            else:\n",
        "                print(f\"Folder not found: {folder_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error in main processing loop. Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "# Loop over all years in the EMO-1–EFAS overlap period\n",
        "for y in {1990..2022}\n",
        "do\n",
        "\tyear=\"$y\"\n",
        "\n",
        "    # File paths for daily maximum/minimum temperature (tx/tn) and outputs\n",
        "\ttx=\"${output_dir}/step_3/emo_data/cutted_emo/tx/EMO-1arcmin-tx_${year}.nc\"\n",
        "\ttn=\"${output_dir}/step_3/emo_data/cutted_emo/tn/EMO-1arcmin-tn_${year}.nc\"\n",
        "\ttasmax=\"${output_dir}/step_3/emo_data/EFAS_converted/tasmax_${year}.nc\"\n",
        "\ttasmin=\"${output_dir}/step_3/emo_data/EFAS_converted/tasmin_${year}.nc\"\n",
        "    tas=\"${output_dir}/step_3/emo_data/EFAS_converted/tas_${year}.nc\"\n",
        "\n",
        "    # Convert EMO-1 tx/tn (°C) to tasmax/tasmin/tas (K)\n",
        "\tcdo -f nc4c -z zip expr,tasmax=\"tx + 273.15\" $tx $tasmax\n",
        "\tcdo -f nc4c -z zip expr,tasmin=\"tn + 273.15\" $tn $tasmin\n",
        "\tcdo -f nc4c -z zip expr,tas=\"(tx + tn) / 2 + 273.15\" -merge $tx $tn $tas\n",
        "\n",
        "    # Relative humidity: derive hurs from vapour pressure (pd) and mean temperature\n",
        "\thurs0=\"${output_dir}/step_3/emo_data/EFAS_converted/hurs_raw_${year}.nc\"\n",
        "\thurs=\"${output_dir}/step_3/emo_data/EFAS_converted/hurs_${year}.nc\"\n",
        "\tpd=\"${output_dir}/step_3/emo_data/cutted_emo/pd/pd_${year}.nc\"\n",
        "\n",
        "    # Compute RH (hurs) from vapour pressure (pd) and mean temperature, then cap at 100%\n",
        "\tcdo -f nc4c -z zip -expr,hurs=\"pd / (6.11 * 10 ^ (7.5 * ((tn+tx)/2) / (237.3+((tn+tx)/2) ) )) * 100\" -merge $tn $tx -shifttime,-1days $pd $hurs0\n",
        "\tcdo -f nc4c -z zip -expr,hurs=\"(hurs > 100 ) ? 100 : hurs\" $hurs0 $hurs\n",
        "\n",
        "    # Near-surface wind speed: copy ws to sfcWind and align time to EMO-1 convention\n",
        "\tws=\"${output_dir}/step_3/emo_data/cutted_emo/ws/ws_${year}.nc\"\n",
        "\tsfcWind=\"${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_${year}.nc\"\n",
        "\n",
        "\tcdo -f nc4c -z zip expr,sfcWind=\"ws\" -shifttime,-1days $ws $sfcWind\n",
        "\n",
        "    # Precipitation: convert from daily totals to daily mean rate (kg m-2 s-1) and shift time\n",
        "\tpr=\"${output_dir}/step_3/emo_data/cutted_emo/pr/pr_${year}.nc\"\n",
        "\tpr_c=\"${output_dir}/step_3/emo_data/EFAS_converted/pr_${year}.nc\"\n",
        "\n",
        "\tcdo -f nc4c -z zip expr,pr=\"pr/86400\" -shifttime,-1days $pr $pr_c\n",
        "\n",
        "    # Shortwave radiation: convert from J m-2 to W m-2 and shift time\n",
        "\trg=\"${output_dir}/step_3/emo_data/cutted_emo/rg/rg_${year}.nc\"\n",
        "\trsds=\"${output_dir}/step_3/emo_data/EFAS_converted/rsds_${year}.nc\"\n",
        "\n",
        "\tcdo -f nc4c -z zip expr,rsds=\"rg/86400\" -shifttime,-1days $rg $rsds\n",
        "\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scripts performs the following actions:\n",
        "\n",
        "1. **Temperature Conversion:**\n",
        "Converts maximum (`tx`) and minimum (`tn`) temperatures from Celsius to Kelvin by adding 273.15.\n",
        "Calculates mean temperature (`tas`) as the average of `tx` and `tn`, then converts to Kelvin.\n",
        "\n",
        "2. **Relative Humidity Calculation:**\n",
        "Computes relative humidity (`hurs`) using temperature and partial pressure data.\n",
        "The calculation uses the Magnus formula for saturation vapor pressure.\n",
        "Limits the relative humidity to a maximum of 100%.\n",
        "\n",
        "3. **Wind Speed Conversion:**\n",
        "Converts wind speed (`ws`) to surface wind (`sfcWind`) without changing values.\n",
        "\n",
        "4. **Precipitation Rate Conversion:**\n",
        "Converts precipitation (`pr`) from mm/day to mm/second by dividing by 86400 (seconds in a day).\n",
        "\n",
        "5. **Solar Radiation Conversion:**\n",
        "Converts global radiation (`rg`) to downward short-wave radiation flux (`rsds`) by dividing by 86400.\n",
        "\n",
        "6. **Time Adjustment:**\n",
        "Applies a one-day backward time shift to certain variables (`hurs`, `sfcWind`, `pr`, `rsds`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4. Data merging\n",
        "<a id=\"Step-4-merging\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the data has been pre-processed for ERA5-Land and EMO-1. Subsequently, the data will now be merged as merging of these files allows for easier handling of long-term climate data series, which is crucial for ease in running simulations in later stages of the process.\n",
        "\n",
        "For this purpose, the script below is designed to consolidate climate data from different sources and time periods. It creates three merged files: two for ERA5-Land data (covering 1950-1989 and 1990-2022) and one for EMO-1 data (covering 1990-2022)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Define the base directory for the output as a variable\n",
        "STEP_4_OUTPUT=\"${output_dir}/step_4\"\n",
        "MERGE_ERA5=\"${STEP_4_OUTPUT}/merge_era5\"\n",
        "MERGE_EMO1=\"${STEP_4_OUTPUT}/merge_emo1\"\n",
        "\n",
        "# Create the output directories if they don't exist\n",
        "mkdir -p $MERGE_ERA5\n",
        "mkdir -p $MERGE_EMO1\n",
        "\n",
        "## Merge files per variable for ERA5_Land; convert the script to cover all vars: tas, tasmin, tasmax, sfcWind, hurs, rsds, pr\n",
        "\n",
        "## Merge ERA5_Land for the period which is available in EMO-1, i.e. 1990-2022\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_2022.nc $MERGE_ERA5/hurs_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_2022.nc $MERGE_ERA5/tas_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_2022.nc $MERGE_ERA5/tasmin_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_2022.nc $MERGE_ERA5/tasmax_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_2022.nc $MERGE_ERA5/sfcWind_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_2022.nc $MERGE_ERA5/rsds_ERA5_1990_2022_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_199?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_200?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_201?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_2020.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_2021.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_2022.nc $MERGE_ERA5/pr_ERA5_1990_2022_t.nc\n",
        "\n",
        "## Merge ERA5_Land for the preceding period which is NOT available in EMO-1, i.e. 1950-1989\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_198?.nc $MERGE_ERA5/hurs_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/tas_ERA5_198?.nc $MERGE_ERA5/tas_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/tasmin_ERA5_198?.nc $MERGE_ERA5/tasmin_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/tasmax_ERA5_198?.nc $MERGE_ERA5/tasmax_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/sfcWind_ERA5_198?.nc $MERGE_ERA5/sfcWind_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/rsds_ERA5_198?.nc $MERGE_ERA5/rsds_ERA5_1950_1989_t.nc\n",
        "cdo mergetime ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_195?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_196?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_197?.nc ${output_dir}/step_2/ERA5_land_daily/pr_ERA5_198?.nc $MERGE_ERA5/pr_ERA5_1950_1989_t.nc\n",
        "\n",
        "## Merge EMO-1 files using relative paths\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/hurs_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/hurs_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/hurs_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/hurs_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/hurs_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/hurs_2022.nc $MERGE_EMO1/hurs_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/tas_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tas_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tas_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tas_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/tas_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/tas_2022.nc $MERGE_EMO1/tas_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmin_2022.nc $MERGE_EMO1/tasmin_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/tasmax_2022.nc $MERGE_EMO1/tasmax_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/sfcWind_2022.nc $MERGE_EMO1/sfcWind_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/rsds_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/rsds_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/rsds_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/rsds_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/rsds_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/rsds_2022.nc $MERGE_EMO1/rsds_1990_2022_t.nc\n",
        "cdo -f nc4c -z zip mergetime ${output_dir}/step_3/emo_data/EFAS_converted/pr_199?.nc ${output_dir}/step_3/emo_data/EFAS_converted/pr_200?.nc ${output_dir}/step_3/emo_data/EFAS_converted/pr_201?.nc ${output_dir}/step_3/emo_data/EFAS_converted/pr_2020.nc ${output_dir}/step_3/emo_data/EFAS_converted/pr_2021.nc ${output_dir}/step_3/emo_data/EFAS_converted/pr_2022.nc $MERGE_EMO1/pr_1990_2022_t.nc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **ERA5-Land Data Merging (1990-2022)**:\n",
        "\n",
        "Merges ERA5-Land relative humidity data files for the years 1990 to 2022.\n",
        "Uses wildcard patterns (e.g., '199?.nc') to include all files for each decade.\n",
        "The merged output is saved as, e.g: 'hurs_ERA5_1990_2022_t.nc'.\n",
        "\n",
        "2. **ERA5-Land Data Merging (1950-1989)**:\n",
        "\n",
        "Merges ERA5-Land relative humidity data files for the years 1950 to 1989.\n",
        "This covers the period not available in EMO-1 dataset.\n",
        "The merged output is saved as, e.g: 'hurs_ERA5_1950_1989_t.nc'.\n",
        "\n",
        "3. **EMO-1 Data Merging (1990-2022)**:\n",
        "\n",
        "Merges EMO-1 relative humidity data files for the years 1990 to 2022.\n",
        "Uses the CDO (Climate Data Operators) tool with specific options:\n",
        "\n",
        "'-f nc4c': Specifies NetCDF4 classic format output.\n",
        "'-z zip': Applies zip compression to the output file.\n",
        "The merged output is saved as, e.g: 'hurs_1990_2022_t.nc'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5. Processing of combined ERA5-Land for bias-adjustment\n",
        "<a id=\"Step-5-secondary-processing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The combined ERA5-Land data from step 4 for the year (1950-1989) and (1990-2022) will go through set of procedures to ensure that the dimensions of each file are in-accordance with the requirements of ISIMIP3BASD scripts. For this purpose, the below script will reorder the dimensions of the NetCDF file to lon,lat,time from time,lat,lon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Ensure the output directory for Step 5 exists\n",
        "mkdir -p \"${output_dir}/step_5\"\n",
        "\n",
        "n_lats=10\n",
        "n_lons=10\n",
        "\n",
        "# Number of chunks along latitude and longitude for ncpdq (can be tuned)\n",
        "n_lats=10\n",
        "n_lons=10\n",
        "\n",
        "# Process each variable separately to prepare ERA5-Land input for BASD\n",
        "# ----------------------------------------------------------------------\n",
        "# hurs: compute number of time steps, then re-chunk and reorder dimensions\n",
        "# ----------------------------------------------------------------------\n",
        "#hurs\n",
        "filename=\"${output_dir}/step_4/merge_era5/hurs_ERA5_1990_2022_t.nc\"  #for merged data\n",
        "#filename=\"${output_dir}/step_2/ERA5_land_daily/hurs_ERA5_2023.nc\"  #for selected year\n",
        "n_times=$(cdo ntime $filename | awk 'NR==1 {print $1}')\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,\"$n_times\" --cnk_dmn=lat,\"$n_lats\" --cnk_dmn=lon,\"$n_lons\" -a lon,lat,time \"$filename\" \"${output_dir}/step_5/ERA5_$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "echo \"n_times: $n_times\"\n",
        "echo \"n_lats: $n_lats\"\n",
        "echo \"n_lons: $n_lons\"\n",
        "echo \"Filename: $filename\"\n",
        "echo \"Output file: ${output_dir}/step_5/ERA5_$(basename \"$filename\" .nc)\"\n",
        "\n",
        "#\n",
        "# ----------------------------------------------------------------------\n",
        "# tas: re-chunk 2 m temperature (later renamed from 2t to tas with ncrename)\n",
        "# ----------------------------------------------------------------------\n",
        "#tas\n",
        "##### After generate file remember to rename variable name to tas, same as EMO1\n",
        "##### use this code in terminal: ncrename -v 2t,tas /mnt/g/compass/compass_framework/step_5/tas_YEAR.nc #replace input file with the tas file that you want to rename\n",
        "\n",
        "filename2=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tas_ERA5_1990_2022_t.nc\"\n",
        "n_times=$(cdo ntime $filename2 | awk 'NR==1 {print $1}')\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename2 \"${output_dir}/step_5/ERA5_$(basename \"$filename2\" .nc).nc\"\n",
        "#\n",
        "echo \"n_times: $n_times\"\n",
        "echo \"n_lats: $n_lats\"\n",
        "echo \"n_lons: $n_lons\"\n",
        "echo \"Filename: $filename2\"\n",
        "echo \"Output file: ${output_dir}/step_5/ERA5_$(basename \"$filename2\" .nc)\"\n",
        "\n",
        "\n",
        "#\n",
        "# ----------------------------------------------------------------------\n",
        "# sfcWind: re-chunk near-surface wind speed\n",
        "# ----------------------------------------------------------------------\n",
        "##sfcWind\n",
        "filename3=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/sfcWind_ERA5_1990_2022_t.nc\"\n",
        "n_times=$(cdo ntime $filename3 | awk 'NR==1 {print $1}')\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename3 \"${output_dir}/step_5/ERA5_$(basename \"$filename3\" .nc).nc\"\n",
        "#\n",
        "echo \"n_times: $n_times\"\n",
        "echo \"n_lats: $n_lats\"\n",
        "echo \"n_lons: $n_lons\"\n",
        "echo \"Filename: $filename3\"\n",
        "echo \"Output file: ${output_dir}/step_5/ERA5_$(basename \"$filename3\" .nc)\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# rsds: re-chunk shortwave radiation\n",
        "# ----------------------------------------------------------------------\n",
        "#rsds\n",
        "filename4=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/rsds_ERA5_1990_2022_t.nc\"\n",
        "n_times=$(cdo ntime $filename4 | awk 'NR==1 {print $1}')\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename4 \"${output_dir}/step_5/ERA5_$(basename \"$filename4\" .nc).nc\"\n",
        "#\n",
        "echo \"n_times: $n_times\"\n",
        "echo \"n_lats: $n_lats\"\n",
        "echo \"n_lons: $n_lons\"\n",
        "echo \"Filename: $filename4\"\n",
        "echo \"Output file: ${output_dir}/step_5/ERA5_$(basename \"$filename4\" .nc)\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# pr: re-chunk precipitation\n",
        "# ----------------------------------------------------------------------\n",
        "#pr\n",
        "filename5=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/pr_ERA5_1990_2022_t.nc\"\n",
        "n_times=$(cdo ntime \"$filename5\" | awk 'NR==1 {print $1}')\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename5 \"${output_dir}/step_5/ERA5_$(basename \"$filename5\" .nc).nc\"\n",
        "\n",
        "echo \"n_times: $n_times\"\n",
        "echo \"n_lats: $n_lats\"\n",
        "echo \"n_lons: $n_lons\"\n",
        "echo \"Filename: $filename5\"\n",
        "echo \"Output file: ${output_dir}/step_5/ERA5_$(basename \"$filename5\" .nc)\"\n",
        "\n",
        "\n",
        "#\n",
        "# ----------------------------------------------------------------------\n",
        "# Derived variables: tasrange and tasskew from tas, tasmin (tn) and tasmax (tx)\n",
        "# ----------------------------------------------------------------------\n",
        "#Convert tas to tasrange and tasskew\n",
        "tas=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tas_ERA5_1990_2022_t.nc\"\n",
        "tn=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tasmin_ERA5_1990_2022_t.nc\"\n",
        "tx=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tasmax_ERA5_1990_2022_t.nc\"\n",
        "tasrange=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tasrange_ERA5_1990_2022_t.nc\"\n",
        "tasskew=\"${output_dir}/step_4/merge_era5/ERA5_land_daily/tasskew_ERA5_1990_2022_t.nc\"\n",
        "# Use the tas file to determine the number of time steps for tasrange/tasskew\n",
        "n_times=$(cdo ntime \"$tas\" | awk 'NR==1 {print $1}')\n",
        "##\n",
        "cdo expr,tasrange=\"tx - tn\" -merge -chname,2t,tn $tn -chname,2t,tx $tx $tasrange\n",
        "cdo expr,tasskew=\" ( tas - tn ) / ( tx - tn ) \" -merge -chname,2t,tn $tn -chname,2t,tx $tx -chname,2t,tas $tas $tasskew\n",
        "##\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $tasrange \"${output_dir}/step_5/ERA5_$(basename \"$tasrange\" .nc).nc\"\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $tasskew \"${output_dir}/step_5/ERA5_$(basename \"$tasskew\" .nc).nc\"\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The script also generates new variables for the temperature calculations (`tasrange` & `tasskew`):\n",
        "\n",
        "a. `tasrange`: Difference between maximum and minimum temperatures.\n",
        "\n",
        "b. `tasskew`: Normalized temperature, indicating where the mean temperature falls between the daily minimum and maximum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6. Processing of combined EMO-1 data\n",
        "<a id=\"Step-6-secondary-processing-emo-1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The combined EMO-1 data from step 4 will be scaled to the same resolution as ERA5-Land to ensure consistency in bias_adjustment and downscaling procedure. The procedure involves generating a grid file with of ERA5Land and remapping the generated weight file from EMO-1 file based on the grid_file. Finally, an aggregated file is generated which is remapped based on the grid_file from ERA5-Land ensuring similar resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Ensure the output directory for Step 6 exists\n",
        "mkdir -p \"${output_dir}/step_6\"\n",
        "\n",
        "# Convert high-resolution EMO-1 fields to the ERA5-Land grid (same resolution)\n",
        "\n",
        "# For each variable, derive the target grid description from the merged ERA5-Land file.\n",
        "# The resulting grid file is reused for EMO-1 remapping for that variable and period.\n",
        "\n",
        "# List of variables to be remapped (can be adapted by the user)\n",
        "variables=(\"tas\" \"pr\" \"rsds\" \"sfcWind\" \"tasmax\" \"tasmin\" \"hurs\")\n",
        "\n",
        "## Loop to process each variable\n",
        "for var in \"${variables[@]}\"; do\n",
        "    # ERA5-Land file used to define the target grid\n",
        "    era5_name=\"${output_dir}/step_4/merge_era5/${var}_ERA5_1990_2022_t.nc\"  #file from step 4 or if it is only one year from step 2\n",
        "    grid_file=\"${output_dir}/step_4/merge_era5/${var}_ERA5_1990_2022_t_aggregate.txt\"\n",
        "    cdo griddes $era5_name > $grid_file\n",
        "\n",
        "    # Generate conservative remapping weights from EMO-1 native grid to the ERA5-Land grid\n",
        "    emo1_file=\"${output_dir}/step_4/merge_emo1/${var}_1990_2022_t.nc\"\n",
        "    weight_file=\"${output_dir}/step_4/merge_emo1/remap_weight_${var}_1990_2022_t_aggregate.nc\"\n",
        "    efas_grid=\"${output_dir}/step_4/merge_emo1/efas_grid.txt\"\n",
        "    cdo gencon,$grid_file -setgrid,$efas_grid $emo1_file $weight_file\n",
        "\n",
        "    # Apply the conservative remapping to EMO-1 and write the remapped file\n",
        "    cdo remap,$grid_file,$weight_file $emo1_file \"${output_dir}/step_6/${var}_1990_2022_t_aggregate.nc\"\n",
        "done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7. Processing of combined EMO-1 data for bias adjustment\n",
        "<a id=\"Step-7-secondary-processing-emo-1-ba\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Repeating the bias adjustment processing procedure as described in step 5 for EMO-1 data now. Running the script below will convert the dimensions of EMO-1 data to proper format ready for bias adjustment and downscaling.\n",
        "\n",
        "### Use only for merged data (for example 1990-2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Ensure the output directory for Step 7 exists\n",
        "mkdir -p \"${output_dir}/step_7\"\n",
        "\n",
        "# Number of chunks along latitude and longitude for ncpdq\n",
        "n_lats=10\n",
        "n_lons=10\n",
        "\n",
        "# Variables to process\n",
        "variables=(\"tas\" \"sfcWind\" \"hurs\" \"rsds\" \"pr\")\n",
        "\n",
        "# Two data types:\n",
        "#  - \"\"          : non-aggregated daily files (..._1990_2022_t.nc, from Step 4)\n",
        "#  - \"aggregate\" : aggregated daily files     (..._1990_2022_t_aggregate.nc, from Step 6)\n",
        "types=(\"aggregate\" \"\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1) Re-chunk core EMO-1 variables for both non-aggregated and aggregated data\n",
        "# ----------------------------------------------------------------------\n",
        "for type in \"${types[@]}\"; do\n",
        "    for var in \"${variables[@]}\"; do\n",
        "        if [ -n \"$type\" ]; then\n",
        "            # Aggregated EMO-1 file from Step 6 (remapped to ERA5-Land grid)\n",
        "            filename=\"${output_dir}/step_6/${var}_1990_2022_t_${type}.nc\"\n",
        "        else\n",
        "            # Non-aggregated EMO-1 file from Step 4 (merged original EMO-1 grid)\n",
        "            filename=\"${output_dir}/step_4/merge_emo1/${var}_1990_2022_t.nc\"\n",
        "        fi\n",
        "\n",
        "        if [ -f \"$filename\" ]; then\n",
        "            echo \"Processing variable: $var, type: ${type:-non-aggregate}\"\n",
        "            # Determine the number of time steps for chunking\n",
        "            n_times=$(cdo ntime \"$filename\" | awk 'NR==1 {print $1}')\n",
        "            echo \"  n_times: $n_times, n_lats: $n_lats, n_lons: $n_lons\"\n",
        "            echo \"  Input file:  $filename\"\n",
        "            echo \"  Output file: ${output_dir}/step_7/EFAS_$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "            # Re-chunk and reorder dimensions to lon,lat,time\n",
        "            ncpdq -4 -O --cnk_plc=g3d \\\n",
        "                  --cnk_dmn=time,\"$n_times\" --cnk_dmn=lat,\"$n_lats\" --cnk_dmn=lon,\"$n_lons\" \\\n",
        "                  -a lon,lat,time \"$filename\" \\\n",
        "                  \"${output_dir}/step_7/EFAS_$(basename \"$filename\" .nc).nc\"\n",
        "        else\n",
        "            echo \"File $filename does not exist, skipping...\"\n",
        "        fi\n",
        "    done\n",
        "done\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2) Derive tasrange and tasskew and re-chunk them for both data types\n",
        "# ----------------------------------------------------------------------\n",
        "for type in \"${types[@]}\"; do\n",
        "    if [ -n \"$type\" ]; then\n",
        "        # Aggregated EMO-1 files from Step 6\n",
        "        tas=\"${output_dir}/step_6/tas_1990_2022_t_${type}.nc\"\n",
        "        tn=\"${output_dir}/step_6/tasmin_1990_2022_t_${type}.nc\"\n",
        "        tx=\"${output_dir}/step_6/tasmax_1990_2022_t_${type}.nc\"\n",
        "        tasrange=\"${output_dir}/step_6/tasrange_1990_2022_t_${type}.nc\"\n",
        "        tasskew=\"${output_dir}/step_6/tasskew_1990_2022_t_${type}.nc\"\n",
        "    else\n",
        "        # Non-aggregated EMO-1 files from Step 4\n",
        "        tas=\"${output_dir}/step_4/merge_emo1/tas_1990_2022_t.nc\"\n",
        "        tn=\"${output_dir}/step_4/merge_emo1/tasmin_1990_2022_t.nc\"\n",
        "        tx=\"${output_dir}/step_4/merge_emo1/tasmax_1990_2022_t.nc\"\n",
        "        tasrange=\"${output_dir}/step_4/merge_emo1/tasrange_1990_2022_t.nc\"\n",
        "        tasskew=\"${output_dir}/step_4/merge_emo1/tasskew_1990_2022_t.nc\"\n",
        "    fi\n",
        "\n",
        "    if [ -f \"$tn\" ] && [ -f \"$tx\" ] && [ -f \"$tas\" ]; then\n",
        "        echo \"Deriving tasrange and tasskew for type: ${type:-non-aggregate}\"\n",
        "\n",
        "        # Calculate tasrange: absolute difference between tasmax and tasmin\n",
        "        cdo -expr,tasrange=\"(( tasmax - tasmin ) > 0 ) ? ( tasmax - tasmin ) : ( tasmin - tasmax )\" \\\n",
        "            -merge \"$tn\" \"$tx\" \"$tasrange\"\n",
        "\n",
        "        # Calculate tasskew: relative position of tas between tasmin and tasmax\n",
        "        cdo -expr,tasskew=\" ( tas - tasmin ) / ( tasmax - tasmin ) \" \\\n",
        "            -merge \"$tn\" \"$tx\" \"$tas\" \"$tasskew\"\n",
        "\n",
        "        # Re-chunk tasrange and tasskew using their own time dimension\n",
        "        for file in \"$tasrange\" \"$tasskew\"; do\n",
        "            if [ -f \"$file\" ]; then\n",
        "                # Determine the number of time steps for this derived file\n",
        "                n_times=$(cdo ntime \"$file\" | awk 'NR==1 {print $1}')\n",
        "                echo \"  Re-chunking derived file: $file (n_times=$n_times)\"\n",
        "\n",
        "                ncpdq -4 -O --cnk_plc=g3d \\\n",
        "                      --cnk_dmn=time,\"$n_times\" --cnk_dmn=lat,\"$n_lats\" --cnk_dmn=lon,\"$n_lons\" \\\n",
        "                      -a lon,lat,time \"$file\" \\\n",
        "                      \"${output_dir}/step_7/EFAS_$(basename \"$file\" .nc).nc\"\n",
        "            else\n",
        "                echo \"  Derived file $file does not exist, skipping...\"\n",
        "            fi\n",
        "        done\n",
        "    else\n",
        "        echo \"Files $tn, $tx or $tas do not exist for type ${type:-non-aggregate}, skipping...\"\n",
        "    fi\n",
        "done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For single-year data workflow (for example 1990 only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Number of netCDF chunks (do not change)\n",
        "n_lats=10\n",
        "n_lons=10\n",
        "\n",
        "# List of variables to process\n",
        "variables=(\"tas\" \"sfcWind\" \"hurs\" \"rsds\" \"pr\")\n",
        "types=(\"aggregate\" \"\")  # 'aggregate' and the second type is empty\n",
        "\n",
        "# Processing files for each variable and data type\n",
        "for type in \"${types[@]}\"; do\n",
        "    for var in \"${variables[@]}\"; do\n",
        "        if [ -n \"$type\" ]; then\n",
        "            filename=\"${output_dir}/step_6/${var}_1990_${type}.nc\"\n",
        "        else\n",
        "            filename=\"${output_dir}/step_3/emo_data/EFAS_converted/${var}_1990.nc\"\n",
        "        fi\n",
        "        \n",
        "        if [ -f \"$filename\" ]; then  # Check if the file exists\n",
        "            n_times=$(cdo ntime \"$filename\" | awk 'NR==1 {print $1}')\n",
        "            ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename \"${output_dir}/step_7/EFAS_$(basename \"$filename\" .nc).nc\"\n",
        "        else\n",
        "            echo \"File $filename does not exist, skipping...\"\n",
        "        fi\n",
        "    done\n",
        "done\n",
        "\n",
        "\n",
        "# Processing tasmin and tasmax files to tasrange and tasskew for both data types\n",
        "for type in \"${types[@]}\"; do\n",
        "    if [ -n \"$type\" ]; then\n",
        "        tas=\"${output_dir}/step_6/tas_1990_${type}.nc\"\n",
        "        tn=\"${output_dir}/step_6/tasmin_1990_${type}.nc\"\n",
        "        tx=\"${output_dir}/step_6/tasmax_1990_${type}.nc\"\n",
        "        tasrange=\"${output_dir}/step_6/tasrange_1990_${type}.nc\"\n",
        "        tasskew=\"${output_dir}/step_6/tasskew_1990_${type}.nc\"\n",
        "    else\n",
        "        tas=\"${output_dir}/step_3/emo_data/EFAS_converted/tas_1990.nc\"\n",
        "        tn=\"${output_dir}/step_3/emo_data/EFAS_converted/tasmin_1990.nc\"\n",
        "        tx=\"${output_dir}/step_3/emo_data/EFAS_converted/tasmax_1990.nc\"\n",
        "        tasrange=\"${output_dir}/step_3/emo_data/EFAS_converted/tasrange_1990.nc\"\n",
        "        tasskew=\"${output_dir}/step_3/emo_data/EFAS_converted/tasskew_1990.nc\"\n",
        "    fi\n",
        "\n",
        "    if [ -f \"$tn\" ] && [ -f \"$tx\" ]; then\n",
        "        cdo -expr,tasrange=\"(( tasmax - tasmin ) > 0 ) ? ( tasmax - tasmin ) : ( tasmin - tasmax )\" -merge $tn $tx $tasrange\n",
        "        cdo -expr,tasskew=\" ( tas - tasmin ) / ( tasmax - tasmin ) \" -merge $tn $tx $tas $tasskew\n",
        "\n",
        "        # Processing tasrange and tasskew files\n",
        "        for file in \"$tasrange\" \"$tasskew\"; do\n",
        "            n_times=$(cdo ntime \"$filename\" | awk 'NR==1 {print $1}')\n",
        "            ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $file \"${output_dir}/step_7/EFAS_$(basename \"$file\" .nc).nc\"\n",
        "        done\n",
        "    else\n",
        "        echo \"Files $tn or $tx do not exist, skipping...\"\n",
        "    fi\n",
        "done\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8. Bias adjustment and statistical downscaling\n",
        "<a id=\"Step-8-secondary-processing-emo-1-ba\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final step is to run the adapted script from Stefan Lange [ISIMIP3BASDv3.0.2](https://zenodo.org/records/7151476) for bias adjustment and statistical downscaling of ERA5-Land data based on finer resolution EMO-1 data that is processed in step 6 combined with the coarser resolution EMO-1 data which was converted to same resolution as ERA5-Land in step 6. By doing so, the ERA5-Land can be downscaled to same resolution as that of EMO-1. \n",
        "\n",
        "Note: The script is time-consuming and may take several days to complete depending on the computational resources. It is not recommended to stop the script during the process as it may corrupt the ouput files. Hence, it is recommended to run it on a hosted server or cloud based services if such resources are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "## this is an adapted script from Stefan Lange for running BASD on ERA5Land and EMO1 files\n",
        "\n",
        "# Variables and calibration period\n",
        "vars=\"hurs pr rsds sfcWind tas tasrange tasskew\"\n",
        "per=\"1990_2022\"\n",
        "\n",
        "# Location of the ISIMIP3BASD code and input/output directories\n",
        "cdir=\"${output_dir}/isimip3basd-master/code\"\n",
        "idir_era5=\"${output_dir}/step_5\"\n",
        "idir_emo=\"${output_dir}/step_7\"\n",
        "odir=\"${output_dir}/step_8\"\n",
        "\n",
        "# Create output directory if it does not exist\n",
        "mkdir -p \"$odir\"\n",
        "\n",
        "# Optional: activate a virtual/conda environment providing ISIMIP3BASD\n",
        "# source /path/to/your/env/bin/activate\n",
        "\n",
        "# Iterate over all variables\n",
        "for var in $vars; do\n",
        "  echo \"===================================================================\"\n",
        "  echo \"Processing variable: $var (period: ${per})\"\n",
        "  echo \"===================================================================\"\n",
        "\n",
        "  # Define file paths\n",
        "  # EMO-1 / EFAS:\n",
        "  #   - obs_hist_fine   : high-resolution reference (non-aggregated)\n",
        "  #   - obs_hist_coarse : aggregated reference on the coarse grid\n",
        "  obs_hist_fine=\"${idir_emo}/EFAS_${var}_${per}_t.nc\"\n",
        "  obs_hist_coarse=\"${idir_emo}/EFAS_${var}_${per}_t_aggregate.nc\"\n",
        "\n",
        "  # ERA5-Land:\n",
        "  #   - sim_hist_coarse : historical model series (overlap with EMO-1)\n",
        "  #   - sim_fut_coarse  : here the same as sim_hist_coarse, since we\n",
        "  #                       currently apply BASD to the 1990–2022 period only\n",
        "  sim_hist_coarse=\"${idir_era5}/ERA5_${var}_ERA5_${per}_t.nc\"\n",
        "  sim_fut_coarse=\"${idir_era5}/ERA5_${var}_ERA5_${per}_t.nc\"\n",
        "\n",
        "  # Output files:\n",
        "  sim_fut_basd_coarse=\"${odir}/ERA5_${var}_ERA5_${per}_t_ba.nc\"\n",
        "  sim_fut_basd_fine=\"${odir}/ERA5_${var}_ERA5_${per}_t_basd.nc\"\n",
        "\n",
        "  echo \"  obs_hist_fine:   $obs_hist_fine\"\n",
        "  echo \"  obs_hist_coarse: $obs_hist_coarse\"\n",
        "  echo \"  sim_hist_coarse: $sim_hist_coarse\"\n",
        "  echo \"  sim_fut_coarse:  $sim_fut_coarse\"\n",
        "  echo \"  sim_fut_ba:      $sim_fut_basd_coarse\"\n",
        "  echo \"  sim_fut_basd:    $sim_fut_basd_fine\"\n",
        "  echo\n",
        "\n",
        "  # Set BASD options based on variable\n",
        "  case $var in\n",
        "    hurs*)\n",
        "      options_ba=\"-v hurs --lower-bound 0 --lower-threshold .01 --upper-bound 100 --upper-threshold 99.99 -t bounded --unconditional-ccs-transfer 1 --trendless-bound-frequency 1\"\n",
        "      options_sd=\"-v hurs --lower-bound 0 --lower-threshold .01 --upper-bound 100 --upper-threshold 99.99\"\n",
        "      ;;\n",
        "    pr*)\n",
        "      options_ba=\"-v pr --lower-bound 0 --lower-threshold .0000011574 --distribution gamma -t mixed\"\n",
        "      options_sd=\"-v pr --lower-bound 0 --lower-threshold .0000011574\"\n",
        "      ;;\n",
        "    rsds*)\n",
        "      options_ba=\"-v rsds --lower-bound 0 --lower-threshold .0001 --upper-bound 1 --upper-threshold .9999 -t bounded -w 15\"\n",
        "      options_sd=\"-v rsds --lower-bound 0 --lower-threshold .01\"\n",
        "      ;;\n",
        "    sfcWind*)\n",
        "      options_ba=\"-v sfcWind --lower-bound 0 --lower-threshold .01 --distribution weibull -t mixed\"\n",
        "      options_sd=\"-v sfcWind --lower-bound 0 --lower-threshold .01\"\n",
        "      ;;\n",
        "    tas)\n",
        "      options_ba=\"-v tas --distribution normal -t additive -d 1\"\n",
        "      options_sd=\"-v tas\"\n",
        "      ;;\n",
        "    tasrange)\n",
        "      options_ba=\"-v tasrange --lower-bound 0 --lower-threshold .01 --distribution weibull -t mixed\"\n",
        "      options_sd=\"-v tasrange --lower-bound 0 --lower-threshold .01\"\n",
        "      ;;\n",
        "    tasskew)\n",
        "      options_ba=\"-v tasskew --lower-bound 0 --lower-threshold .0001 --upper-bound 1 --upper-threshold .9999 -t bounded\"\n",
        "      options_sd=\"-v tasskew --lower-bound 0 --lower-threshold .0001 --upper-bound 1 --upper-threshold .9999\"\n",
        "      ;;\n",
        "    *)\n",
        "      echo \"Variable $var not supported ... aborting ...\"\n",
        "      exit 1\n",
        "      ;;\n",
        "  esac\n",
        "\n",
        "  # -------------------------------------------------------------------\n",
        "  # 1) Bias adjustment on the coarse grid (ERA5-Land resolution)\n",
        "  # -------------------------------------------------------------------\n",
        "  echo \"Running bias_adjustment.py for $var ...\"\n",
        "  time python -u \"${cdir}/bias_adjustment.py\" $options_ba \\\n",
        "    --n-processes 16 \\\n",
        "    --randomization-seed 0 \\\n",
        "    --step-size 1 \\\n",
        "    -o \"$obs_hist_coarse\" \\\n",
        "    -s \"$sim_hist_coarse\" \\\n",
        "    -f \"$sim_fut_coarse\" \\\n",
        "    -b \"$sim_fut_basd_coarse\"\n",
        "\n",
        "  chmod 664 \"$sim_fut_basd_coarse\"\n",
        "  echo\n",
        "\n",
        "  # -------------------------------------------------------------------\n",
        "  # 2) Statistical downscaling to the EMO-1/EFAS grid\n",
        "  # -------------------------------------------------------------------\n",
        "  echo \"Running statistical_downscaling.py for $var ...\"\n",
        "  time python -u \"${cdir}/statistical_downscaling.py\" $options_sd \\\n",
        "    --n-processes 16 \\\n",
        "    --randomization-seed 0 \\\n",
        "    -o \"$obs_hist_fine\" \\\n",
        "    -s \"$sim_fut_basd_coarse\" \\\n",
        "    -f \"$sim_fut_basd_fine\"\n",
        "\n",
        "  chmod 664 \"$sim_fut_basd_fine\"\n",
        "  echo\n",
        "done\n",
        "\n",
        "# Optional: deactivate environment\n",
        "# deactivate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9. Convert Bias-adjusted and downscaled ERA5 to EMO-1 format\n",
        "<a id=\"step-9-convert-bias-adjusted\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9.1\n",
        "\n",
        "The script will convert BASD ERA5-Land to EMO-1 format from 1950-1989 for every variable. If you are facing memory issues, refer to possible fixes section 1.4.\n",
        "\n",
        "Use this script only for merged data (for example 1990-2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "opath=\"${output_dir}/step_9\"\n",
        "\n",
        "## Convert BASD back to EMO-1 format; adapt the script to cover also the 1950-1989 period\n",
        "\n",
        "# Wind\n",
        "filename=\"${output_dir}/step_8/ERA5_sfcWind_ERA5_1950_1989_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {1950..1989}\n",
        "do\n",
        "  oname2=\"${opath}ws_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,ws=\"sfcWind\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1 || { echo \"Failed to remove $oname1\"; exit 1; }\n",
        "\n",
        "# Temperature\n",
        "tas=\"${output_dir}/step_8/ERA5_tas_ERA5_1950_1989_t_basd.nc\"\n",
        "tasrange=\"${output_dir}/step_8/ERA5_tasrange_ERA5_1950_1989_t_basd.nc\"\n",
        "tas_a=\"${opath}t_$(basename \"$tas\" .nc).nc\"\n",
        "tasrange_a=\"${opath}t_$(basename \"$tasrange\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tas $tas_a\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tasrange $tasrange_a\n",
        "for y in {1950..1989}\n",
        "do\n",
        "  tas_b=\"${opath}ta_$(basename \"$tas\" .nc)_${y}.nc\"\n",
        "  tasrange_b=\"${opath}ta_$(basename \"$tasrange\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tas_a $tas_b\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tasrange_a $tasrange_b\n",
        "  tx=\"${opath}tx_${y}.nc\"\n",
        "  tn=\"${opath}tn_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,tx=\"tas + 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tx\n",
        "  cdo -L -f nc4c -z zip expr,tn=\"tas - 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tn\n",
        "  rm $tasrange_b # don't remove tas_b, needed later\n",
        "done\n",
        "rm $tas_a\n",
        "rm $tasrange_a\n",
        "\n",
        "# Radiation\n",
        "filename=\"${output_dir}/step_8/ERA5_rsds_ERA5_1950_1989_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {1950..1989}\n",
        "do\n",
        "  oname2=\"${opath}rg_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,rg=\"rsds * 86400\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1\n",
        "\n",
        "# Vapour pressure / humidity\n",
        "filename=\"${output_dir}/step_8/ERA5_hurs_ERA5_1950_1989_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {1950..1989}\n",
        "do\n",
        "  oname2=\"${opath}ta_$(basename \"$filename\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $oname1 $oname2\n",
        "  oname3=\"${opath}pd_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,pd=\"hurs / 100 * (6.11 * 10 ^ (7.5 * (tas - 273.15) / (237.3 + (tas - 273.15))))\" -merge -shifttime,1days -selyear,$y $oname2 -shifttime,630minutes $tas_b $oname3\n",
        "done\n",
        "rm $oname1\n",
        "\n",
        "# Precipitation\n",
        "filename=\"${output_dir}/step_8/ERA5_pr_ERA5_1950_1989_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {1950..1989}\n",
        "do\n",
        "  oname2=\"${opath}pr_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip mulc,86400 -shifttime,24hours -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9.2\n",
        "\n",
        "The script will convert BASD ERA5-Land to EMO-1 format from 1990-2022.\n",
        "\n",
        "Use this script only for merged data (for example 1990-2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "opath=\"${output_dir}/step_9\"\n",
        "\n",
        "## convert BASD back to EMO-1 format; adapt the script to cover also the 1990-2022\n",
        "\n",
        "# Wind\n",
        "filename=\"${output_dir}/step_8/ERA5_sfcWind_ERA5_1990_2022_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {1990..2022}\n",
        "do\n",
        "  oname2=\"${opath}ws_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,ws=\"sfcWind\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1 || { echo \"Failed to remove $oname1\"; exit 1; }\n",
        "\n",
        "# Temperature\n",
        "tas=\"${output_dir}/step_8/ERA5_tas_ERA5_1990_2022_t_basd.nc\"\n",
        "tasrange=\"${output_dir}/step_8/ERA5_tasrange_ERA5_1990_2022_t_basd.nc\"\n",
        "tas_a=\"${opath}t_$(basename \"$tas\" .nc).nc\"\n",
        "tasrange_a=\"${opath}t_$(basename \"$tasrange\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tas $tas_a\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tasrange $tasrange_a\n",
        "for y in {1990..2022}\n",
        "do\n",
        "  tas_b=\"${opath}ta_$(basename \"$tas\" .nc)_${y}.nc\"\n",
        "  tasrange_b=\"${opath}ta_$(basename \"$tasrange\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tas_a $tas_b\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tasrange_a $tasrange_b\n",
        "  tx=\"${opath}tx_${y}.nc\"\n",
        "  tn=\"${opath}tn_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,tx=\"tas + 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tx\n",
        "  cdo -L -f nc4c -z zip expr,tn=\"tas - 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tn\n",
        "  rm $tasrange_b # don't remove tas_b, needed later\n",
        "done\n",
        "rm $tas_a\n",
        "rm $tasrange_a\n",
        "\n",
        "## Radiation\n",
        "filename=\"${output_dir}/step_8/ERA5_rsds_ERA5_1990_2022_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {1990..2022}\n",
        "do\n",
        "  oname2=\"${opath}rg_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,rg=\"rsds * 86400\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1\n",
        "\n",
        "## Vapour pressure / humidity\n",
        "filename=\"${output_dir}/step_8/ERA5_hurs_ERA5_1990_2022_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {1990..2022}\n",
        "do\n",
        "  oname2=\"${opath}ta_$(basename \"$filename\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $oname1 $oname2\n",
        "  oname3=\"${opath}pd_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,pd=\"hurs / 100 * (6.11 * 10 ^ (7.5 * (tas - 273.15) / (237.3+ (tas - 273.15) ) ))\" -merge -shifttime,1days -selyear,$y $oname2 -shifttime,630minutes $tas_b $oname3\n",
        "done\n",
        "rm $oname1\n",
        "\n",
        "## Precipitation\n",
        "filename=\"${output_dir}/step_8/ERA5_pr_ERA5_1990_2022_t_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {1990..2022}\n",
        "do\n",
        "  oname2=\"${opath}pr_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip mulc,86400 -shifttime,24hours -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9.3 (optional for a single year, for example 2023)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "opath=\"${output_dir}/step_9/\"\n",
        "\n",
        "###################################### \n",
        "#            1\n",
        "######################################\n",
        " Temperature\n",
        "tas=\"${output_dir}/step_8/ERA5_tas_ERA5_2023_basd.nc\"\n",
        "tasrange=\"${output_dir}/step_8/ERA5_tasrange_ERA5_2023_basd.nc\"\n",
        "tas_a=\"${opath}t_$(basename \"$tas\" .nc).nc\"\n",
        "tasrange_a=\"${opath}t_$(basename \"$tasrange\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tas $tas_a\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $tasrange $tasrange_a\n",
        "for y in {2023..2023}\n",
        "do\n",
        "  tas_b=\"${opath}ta_$(basename \"$tas\" .nc)_${y}.nc\"\n",
        "  tasrange_b=\"${opath}ta_$(basename \"$tasrange\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tas_a $tas_b\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $tasrange_a $tasrange_b\n",
        "  tx=\"${opath}tx_${y}.nc\"\n",
        "  tn=\"${opath}tn_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,tx=\"tas + 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tx\n",
        "  cdo -L -f nc4c -z zip expr,tn=\"tas - 0.5 * tasrange - 273.15\" -merge $tas_b $tasrange_b $tn\n",
        "  rm $tasrange_b # don't remove tas_b, needed later\n",
        "done\n",
        "rm $tas_a\n",
        "rm $tasrange_a\n",
        "\n",
        "\n",
        "# Vapour pressure / humidity\n",
        "filename=\"${output_dir}/step_8/ERA5_hurs_ERA5_2023_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {2023..2023}\n",
        "do\n",
        "  oname2=\"${opath}ta_$(basename \"$filename\" .nc)_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip -selyear,$y $oname1 $oname2\n",
        "  oname3=\"${opath}pd_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,pd=\"hurs / 100 * (6.11 * 10 ^ (7.5 * (tas - 273.15) / (237.3+ (tas - 273.15) ) ))\" -merge -shifttime,1days -selyear,$y $oname2 -shifttime,630minutes $tas_b $oname3\n",
        "done\n",
        "rm $oname1\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "###################################### \n",
        "#            2\n",
        "######################################\n",
        "\n",
        "# Radiation\n",
        "filename=\"${output_dir}/step_8/ERA5_rsds_ERA5_2023_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {2023..2023}\n",
        "do\n",
        "  oname2=\"${opath}rg_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,rg=\"rsds * 86400\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1\n",
        "\n",
        "# Wind\n",
        "filename=\"${output_dir}/step_8/ERA5_sfcWind_ERA5_2023_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "\n",
        "for y in {2023..2023}\n",
        "do\n",
        "  oname2=\"${opath}ws_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip expr,ws=\"sfcWind\" -shifttime,1days -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1 || { echo \"Failed to remove $oname1\"; exit 1; }\n",
        "\n",
        "#\n",
        "# Precipitation\n",
        "filename=\"${output_dir}/step_8/ERA5_pr_ERA5_2023_basd.nc\"\n",
        "oname1=\"${opath}t_$(basename \"$filename\" .nc).nc\"\n",
        "ncpdq -O --cnk_plc=uck -a time,lat,lon $filename $oname1\n",
        "for y in {2023..2023}\n",
        "do\n",
        "  oname2=\"${opath}pr_${y}.nc\"\n",
        "  cdo -L -f nc4c -z zip mulc,86400 -shifttime,24hours -selyear,$y $oname1 $oname2\n",
        "done\n",
        "rm $oname1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10. Final post-processing of ERA-5 files for consistency with EMO-1 data\n",
        "<a id=\"step-10-final-post-processing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://naturalhazards.eu/timeshift.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10.1\n",
        "The script defines input, output, and intermediate file paths for the processing of meteorological variables (1950 to 2022). The ipath, epath, and opath variables designate paths for the input files, ERA5 climate data, and output files, respectively.\n",
        "\n",
        "The script performs several tasks in a loop over each year. It starts by computing a land mask for the BASD dataset, then extracts grid information and calculates regridding weights for the ERA5 data. For each year, the script processes the variable files by setting up file names for input and output. It handles the regridding of ERA5 files to match the BASD grid, adjusting for time shifts where necessary. It converts relative humidity (hurs) into partial pressure (pd) for certain variables and ensures consistency in units and time shifts across files.\n",
        "\n",
        "The script also corrects the time vector, processes and compresses data using cdo commands for specific meteorological variables, and applies transformations such as packing values into smaller byte formats and adjusting chunking for NetCDF outputs. The time units are adapted depending on the time period (1950-1989 or 1990-2022), and temporary files are removed at the end of each iteration.\n",
        "\n",
        "The script requires specific adjustments for each variable where the user needs to add some expressions and different timestamps to modify the data to match with that of EMO1. The code snippets for each variable has been provided seperately below for convenience.\n",
        "\n",
        "The below script is for `ws` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}ws_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask.nc\"\n",
        "cdo -f nc4c -z zip setmisstoc,0 -expr,sfcWind=\"(ws >= 0) ? 1 : 0\" -seltimestep,1 $basd_file $basd_mask\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}tasmax_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=990\n",
        "n_lons=1510\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..2020}\n",
        "do\n",
        "\tfilename=\"${ipath}ws_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}sfcWind_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_sfcWind_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -shifttime,1days $era5_file $era5_file_regrid\n",
        "\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,-330minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,ws,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,ws,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'ws=pack(ws,0.1,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-24)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1990-01-01 00:00:00\" $oname5\n",
        "\t## 1950-1989: ncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\n",
        "  # remove temporary files\n",
        "\trm t*_ws_*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify if the time steps are correct, use command cdo info {path_to_nc.nc} and do the same for EMO-1 file to confirm that the timestamps are consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10.2\n",
        "The initial code needs to be modified with expr command and other modifications necessary to add consistency in the units between EMO-1 and ERA-5 data. Therefore, below are the changes that need to be made to ensure that every variable is gap-filled and processed without any errors.\n",
        "\n",
        "`pd`:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}pd_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask_pl.nc\"\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}hurs_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=142\n",
        "n_lons=262\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..1989}\n",
        "do\n",
        "\tfilename=\"${ipath}pd_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}hurs_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_hurs_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "  \tera5_file_tas=\"${epath}tas_ERA5_${tile}.nc\"\n",
        "\tera5_file_pd=\"${epath}pd_ERA5_${tile}.nc\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tAdded expr,pd=\"hurs / 100 * (6.11 * 10 ^ (7.5 * (dpt - 273.15) / (237.3 + (dpt - 273.15) ) ))\" \n",
        "\tto calculate the actual vapor pressure which is used in EMO-1 data.\n",
        "\n",
        "\t\"\"\"\n",
        "\tcdo -f nc4c -z zip expr,pd=\"hurs / 100 * (6.11 * 10 ^ (7.5 * (dpt - 273.15) / (237.3 + (dpt - 273.15) ) ))\" -merge $era5_file $era5_file_tas $era5_file_pd\n",
        "\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -shifttime,1days $era5_file_pd $era5_file_regrid\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,-690minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,pd,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,pd,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'pd=pack(pd,0.1,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-24)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\t#ncatted -a units,time,o,c,\"days since 1990-01-01 00:00:00\" $oname5\n",
        "\t\n",
        "  # remove temporary files\n",
        "rm t*_pd*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pr`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}pr_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask_pl.nc\"\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}pr_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=142\n",
        "n_lons=262\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..1989}\n",
        "do\n",
        "\tfilename=\"${ipath}pr_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}pr_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_pr_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tAdded -expr,pr=\"pr*86400\" for consistency between the units in EMO-1 and ERA-5\n",
        "\t\n",
        "\t\"\"\"\n",
        "\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -expr,pr=\"pr*86400\" -shifttime,1days $era5_file $era5_file_regrid\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,360minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,pr,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,pr,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'pr=pack(pr,0.1,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-24)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\t#ncatted -a units,time,o,c,\"days since 1990-01-01 00:00:00\" $oname5\n",
        "\n",
        "  # remove temporary files\n",
        "rm t*_pr_*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`rg`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}rg_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask_pl.nc\"\n",
        "#cdo -f nc4c -z zip expr,tas=\"(tn >= 0) ? 1 : 0\" -seltimestep,1 $basd_file $basd_mask\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}rsds_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=142\n",
        "n_lons=262\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..1950}\n",
        "do\n",
        "\tfilename=\"${ipath}rg_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}rsds_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_rsds_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tAdded -expr,rg=\"rg*86400\" for consistency between the units in EMO-1 and ERA-5\n",
        "\t\n",
        "\t\"\"\"\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -expr,rg=\"rg*86400\" -shifttime,1days $era5_file $era5_file_regrid\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,0minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,rg,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,rg,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'rg=pack(rg,10000,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-24)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\t## 1950-1989: ncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\n",
        "  # remove temporary files (optional, can be used to troubleshoot the issues in the code)\n",
        "rm t*_rd_*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`tn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}tn_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask_pl.nc\"\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}tasmin_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=142\n",
        "n_lons=262\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..1989}\n",
        "do\n",
        "\tfilename=\"${ipath}tn_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}tasmin_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_tasmin_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tAdded -expr,tn=\"tx-273.15\" to fix the issue that was made earlier in step 2 which replaced tx with tn and tn with tx.\n",
        "\t\n",
        "\t\"\"\"\n",
        "\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -expr,tn=\"tx-273.15\" $era5_file $era5_file_regrid\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,30minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,tn,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,tn,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'tn=pack(tn,0.1,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-30)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\t## 1950-1989: ncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\n",
        "  # remove temporary files\n",
        "rm t*_tn_*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`tx`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ipath=\"${output_dir}/step_9/\"\n",
        "epath=\"${output_dir}/step_2/ERA5_for_gapfill/\"\n",
        "opath=\"${output_dir}/step_10/\"\n",
        "\n",
        "## compute BASD landmask, grid and weights for regridding ERA% (only needed once, the same files are used for other variables)\n",
        "basd_file=\"${ipath}tx_1951.nc\" # from step 9\n",
        "basd_mask=\"${opath}basd_landmask_pl.nc\"\n",
        "grid_file=\"${opath}basd_grid.txt\"\n",
        "cdo griddes $basd_file > $grid_file\n",
        "weight_file=\"${opath}ERA5_weights.nc\"\n",
        "era5_file=\"${epath}tasmax_ERA5_1951.nc\" # from step 2\n",
        "cdo gennn,$grid_file $era5_file $weight_file\n",
        "\n",
        "# dimension (adapt n_lats and n_longs to file dimensions divided by 3)\n",
        "n_times=1\n",
        "n_lats=142\n",
        "n_lons=262\n",
        "\n",
        "## loop to repeat per variable\n",
        "for y in {1950..1989}\n",
        "do\n",
        "\tfilename=\"${ipath}tx_${y}.nc\" # from step 9\n",
        "\ttile=\"$(echo $(basename \"$filename\" .nc) | cut -d'_' -f2)\"\n",
        "\tera5_file=\"${epath}tasmax_ERA5_${tile}.nc\" # from step 2\n",
        "\tera5_file_regrid=\"${epath}regrid_tasmax_ERA5_${tile}.nc\"\n",
        "\toname1=\"${opath}tb_$(basename \"$filename\" .nc).nc\"\n",
        "\toname2=\"${opath}tc_$(basename \"$filename\" .nc).nc\"\n",
        "\toname3=\"${opath}td_$(basename \"$filename\" .nc).nc\"\n",
        "\toname4=\"${opath}te_$(basename \"$filename\" .nc).nc\"\n",
        "\toname5=\"${opath}$(basename \"$filename\" .nc).nc\"\n",
        "\n",
        "\t\"\"\"\n",
        "\tAdded -expr,tn=\"tx-273.15\" to fix the issue that was made earlier in step 2 which replaced tx with tn and tn with tx.\n",
        "\t\n",
        "\t\"\"\"\n",
        "\n",
        "\tcdo -f nc4c -z zip remap,$grid_file,$weight_file -expr,tx=\"tn-273.15\" $era5_file $era5_file_regrid\n",
        "\n",
        "\tcdo -f nc4c -z zip ifthenelse $basd_mask $filename $era5_file_regrid $oname1\n",
        "\tcdo -f nc4c -z zip shifttime,390minutes $oname1 $oname2\n",
        "\n",
        "\tncatted -O -a _FillValue,tx,o,s,-9999 $oname2\n",
        "\tncatted -O -a missing_value,tx,o,s,-9999 $oname2\n",
        "\n",
        "\tncap2 -v -O -s 'tx=pack(tx,0.1,0);' $oname2 $oname3\n",
        "\tncpdq -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a time,lat,lon $oname3 $oname4\n",
        "\tncap2 -O -s 'time=(time-24)/24;' $oname4 $oname5\n",
        "\tncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\t## 1950-1989: ncatted -a units,time,o,c,\"days since 1950-01-02 00:00:00\" $oname5\n",
        "\n",
        "  # remove temporary files\n",
        "rm t*_tx_*.nc\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10.3\n",
        "Calculation of the annual mean. <br/>\n",
        "You need to SHP file with Province coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import xarray as xr\n",
        "import rioxarray\n",
        "import numpy as np\n",
        "from rasterio.features import geometry_mask\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "input_dir='/LOCATION-TO-YOUR-FOLDER/CLIMB'  # Keep location of Python to compass_framework folder\n",
        "\n",
        "# Load the shapefile\n",
        "shapefile_path = '${input_dir}/step_10/shp/voivodeships.shp' \n",
        "poland_shapefile = gpd.read_file(shapefile_path)\n",
        "\n",
        "\n",
        "# List of variables and years you want to process\n",
        "variables = ['tx', 'tn', 'pd', 'ws', 'rg', 'pr']  # 'tx', 'tn', 'pd', 'ws', 'rg', 'pr'\n",
        "years = range(1990, 2022)  # range\n",
        "\n",
        "# Directory where your NetCDF files are located\n",
        "netcdf_dir = '${input_dir}/step_10/'\n",
        "\n",
        "# Directory where you want to save the output CSV files\n",
        "output_dir = '${input_dir}/step_10/mean-y/'\n",
        "\n",
        "# Make sure the output directory exists\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# List to store results for CSV output\n",
        "results = []\n",
        "\n",
        "# Loop through each variable\n",
        "for variable in variables:\n",
        "\n",
        "    # Loop through each year and load the NetCDF file\n",
        "    for year in years:\n",
        "        netcdf_file = f'{netcdf_dir}{variable}_{year}.nc'\n",
        "        \n",
        "        # Check if file exist\n",
        "        if not os.path.exists(netcdf_file):\n",
        "            print(f\"File {netcdf_file} not found, skipping this year.\")\n",
        "            continue\n",
        "        \n",
        "        # Open the dataset\n",
        "        data = xr.open_dataset(netcdf_file)\n",
        "        \n",
        "        # Access the specific variable (e.g., 'tx' for maximum daily temperature)\n",
        "        var_data = data[variable]\n",
        "        \n",
        "        # Make sure the data has proper geospatial coordinates\n",
        "        var_data = var_data.rio.write_crs(\"EPSG:4326\")\n",
        "\n",
        "        # Loop through each voivodeship\n",
        "        for idx, voivodeship in poland_shapefile.iterrows():\n",
        "            # Get the geometry (boundary) of the voivodeship\n",
        "            geometry = [voivodeship['geometry']]\n",
        "            \n",
        "            # Create a mask for the voivodeship geometry (lat/lon grid is 2D)\n",
        "            mask = geometry_mask([geom for geom in geometry], \n",
        "                                 transform=var_data.rio.transform(), \n",
        "                                 invert=True, \n",
        "                                 out_shape=var_data.shape[-2:])\n",
        "            \n",
        "            # Apply the mask to the entire time series data\n",
        "            masked_data = var_data.where(mask)\n",
        "\n",
        "            # Calculate the mean value at each grid cell across all timestamps\n",
        "            if variable == 'pr':\n",
        "                # For 'pr' \n",
        "                mean_per_grid_cell = masked_data.mean(dim='time') * 365\n",
        "            else:\n",
        "                # For all variables except pr\n",
        "                mean_per_grid_cell = masked_data.mean(dim='time')\n",
        "\n",
        "            # Calculate the mean of these values for the entire region within the voivodeship\n",
        "            mean_max_value = mean_per_grid_cell.mean().item()\n",
        "\n",
        "            # Add the result to the list\n",
        "            results.append({\n",
        "                'variable': variable, \n",
        "                'voivodeship': voivodeship['nazwa'],  \n",
        "                'mean': mean_max_value,  \n",
        "                'year': year  \n",
        "            })\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df_results = pd.json_normalize(results)\n",
        "df_results['mean'] = df_results['mean'].round(4)\n",
        "df_results = df_results.dropna(subset=['year'])\n",
        "df_results['year'] = df_results['year'].astype(int).astype(str)\n",
        "\n",
        "print(df_results.head())\n",
        "print(df_results.columns)\n",
        "\n",
        "# Save to CSV, one file per variable\n",
        "for variable in variables:\n",
        "    df_var = df_results[df_results['variable'] == variable]\n",
        "    csv_filename = f'{output_dir}{variable}-mean-y.csv'\n",
        "    df_var.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(\"CSV is ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11 Lisvap script\n",
        "\n",
        "To run Step 11 (LISVAP-based PET calculation), you need to use the external LISVAP tool.\n",
        "LISVAP is not included in this repository because the original source distribution is large (~2 GB).\n",
        "Instead, please install LISVAP and PCRaster from their official sources and then use the sample configuration files provided in this folder.\n",
        "\n",
        "⸻\n",
        "\n",
        "\t1.\tInstall LISVAP and PCRaster\n",
        "\n",
        "⸻\n",
        "\n",
        "Follow the official LISVAP installation guide:\n",
        "\n",
        "https://ec-jrc.github.io/lisflood-lisvap/3_LISVAP_installation/\n",
        "\n",
        "and the official PCRaster installation guide:\n",
        "\n",
        "https://pcraster.geo.uu.nl/pcraster/4.4.1/documentation/pcraster_project/install.html\n",
        "\n",
        "In our workflow, PCRaster is run from a Conda virtual environment (for example named lisvapenv),\n",
        "and LISVAP is installed via pip inside the same environment. Any equivalent setup is fine,\n",
        "as long as LISVAP and PCRaster are both available on your PATH.\n",
        "\n",
        "⸻\n",
        "\n",
        "\t2.\tFiles provided in this folder (step_11)\n",
        "\n",
        "⸻\n",
        "\n",
        "This folder contains:\n",
        "\t•\tconfig.xml\n",
        "A sample LISVAP configuration file adapted to the CLIMB workflow.\n",
        "It already contains the correct structure and parameters for reading the output\n",
        "of Step 10 and producing daily PET fields.\n",
        "\t•\tbasemap/\n",
        "A basemap directory prepared for Poland (e.g. required masks, static maps).\n",
        "LISVAP will use these files for spatial reference and land/sea masking.\n",
        "\n",
        "You can use these files as a starting point and only adjust the paths to match\n",
        "your local installation.\n",
        "\n",
        "⸻\n",
        "\n",
        "\t3.\tHow to adapt the configuration file\n",
        "\n",
        "⸻\n",
        "\n",
        "\t- Copy the config.xml from this step_11 folder into your LISVAP working directory (or point LISVAP directly to this file, depending on how you run it).\n",
        "\t- Open config.xml in a text editor and update the path entries so that they match your system:\n",
        "        •\tpaths to the input meteorological data (NetCDF files produced by Step 10),\n",
        "        •\tpath to the basemap directory (this basemap/ folder for Poland),\n",
        "        •\tpath to the output directory where LISVAP should write PET NetCDF files.\n",
        "\t- Save the modified config.xml.\n",
        "\n",
        "The original example configuration files shipped with LISVAP may not work directly\n",
        "with the CLIMB data structure. For this reason, it is recommended to start from\n",
        "the config.xml provided here and only change the paths.\n",
        "\n",
        "⸻\n",
        "\n",
        "\t4.\tRunning LISVAP\n",
        "\n",
        "⸻\n",
        "\n",
        "After adapting config.xml:\n",
        "\t1.\tActivate the environment where LISVAP and PCRaster are installed (e.g. lisvapenv).\n",
        "\t2.\tRun LISVAP using the adapted configuration file, following the instructions from the\n",
        "LISVAP documentation (for example by specifying the config file as input).\n",
        "\n",
        "Please refer to the official LISVAP manual for the exact command-line invocation,\n",
        "as it may change between versions.\n",
        "\n",
        "⸻\n",
        "\n",
        "\t5.\tNotes\n",
        "\n",
        "⸻\n",
        "\n",
        "•\tThis repository only provides the configuration and example setup for Step 11; it does not redistribute the LISVAP source code or binaries.\n",
        "•\tThe LISVAP step is optional and only needed if you want to derive PET products in a way that is fully consistent with the JRC/LISFLOOD workflow.\n",
        "•\tMake sure that the temporal coverage and spatial grid of your Step 10 outputs are consistent with the configuration used in LISVAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample config XML is located in SCRIPT folder.<br/> \n",
        "\n",
        "Oryginal file from Lisvap project might not work properly. Donwload sample config file from SCRIPT folder and change path to your location. Next replace the downloaded config xml with the one in the script and run LISVAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Possible fixes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section contains information about possible fixes or suggestions to prevent the issues that were encountered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Recommended versions on which all the scripts have been tested to run without any issues.\n",
        "\n",
        "| Software               | Version     |\n",
        "|:-----------------------|:------------|\n",
        "| Python                 | 3.11.6      |\n",
        "| Ubuntu                 | 22.04 ([Check WSL2 documentation](https://learn.microsoft.com/en-us/windows/wsl/install) for Linux on Windows)      |\n",
        "| Cartopy                | 0.23.0      |\n",
        "| cdo                    | 2.2.3       |\n",
        "| cdsapi                 | 0.7.0       |\n",
        "| certifi                | 2024.7.4    |\n",
        "| cf-units               | 3.2.0       |\n",
        "| cftime                 | 1.6.4       |\n",
        "| comm                   | 0.2.2       |\n",
        "| contourpy              | 1.2.1       |\n",
        "| cycler                 | 0.12.1      |\n",
        "| debugpy                | 1.8.5       |\n",
        "| decorator              | 5.1.1       |\n",
        "| executing              | 2.0.1       |\n",
        "| exsce                  | 1.5.0       |\n",
        "| ecCodes                | 2.31.1      |\n",
        "| FILE                   | 1.9.1       |\n",
        "| fonttools              | 4.53.1      |\n",
        "| h5netcdf               | 1.2.0       |\n",
        "| ipykernel              | 6.29.5      |\n",
        "| ipython                | 8.26.0      |\n",
        "| ipywidgets             | 8.1.3       |\n",
        "| jedi                   | 0.19.1      |\n",
        "| Jinja2                 | 3.1.4       |\n",
        "| jupyter_client         | 8.6.2       |\n",
        "| jupyter_core           | 5.7.2       |\n",
        "| jupyterlab_widgets     | 3.0.11      |\n",
        "| kiwisolver             | 1.4.5       |\n",
        "| MarkupSafe             | 2.1.5       |\n",
        "| matplotlib             | 3.9.1.post1 |\n",
        "| matplotlib-inline      | 0.1.7       |\n",
        "| nest-asyncio           | 1.6.0       |\n",
        "| NetCDF                 | 4.9.2       |\n",
        "| netCDF4                | 1.7.1.post1 |\n",
        "| numpy                  | 1.26.4      |\n",
        "| packaging              | 24.1        |\n",
        "| pandas                 | 2.2.2       |\n",
        "| parso                  | 0.8.4       |\n",
        "| pexpect                | 4.9.0       |\n",
        "| pillow                 | 10.4.0      |\n",
        "| pip                    | 24.2        |\n",
        "| platformdirs           | 4.2.2       |\n",
        "| prompt_toolkit         | 3.0.47      |\n",
        "| psutil                 | 6.0.0       |\n",
        "| ptyprocess             | 0.7.0       |\n",
        "| pure_eval              | 0.2.3       |\n",
        "| Pygments               | 2.18.0      |\n",
        "| pyparsing              | 3.1.2       |\n",
        "| pyproj                 | 3.6.1       |\n",
        "| pyshp                  | 2.3.1       |\n",
        "| python-dateutil        | 2.9.0.post0 |\n",
        "| pytz                   | 2024.1      |\n",
        "| pyzmq                  | 26.1.0      |\n",
        "| scipy                  | 1.14.0      |\n",
        "| setuptools             | 65.5.0      |\n",
        "| shapely                | 2.0.5       |\n",
        "| six                    | 1.16.0      |\n",
        "| stack-data             | 0.6.3       |\n",
        "| tabulate               | 0.9.0       |\n",
        "| tornado                | 6.4.1       |\n",
        "| traitlets              | 5.14.3      |\n",
        "| typing_extensions      | 4.12.2      |\n",
        "| tzdata                 | 2024.1      |\n",
        "| wcwidth                | 0.2.13      |\n",
        "| widgetsnbextension     | 4.0.11      |\n",
        "| xarray                 | 2024.7.0    |\n",
        "| lisvap                 | [https://ec-jrc.github.io/lisflood-lisvap/3_LISVAP_installation/]    | \n",
        "| pcraster               | 4.4.1 [https://pcraster.geo.uu.nl/pcraster/4.4.1/documentation/pcraster_project/install.html]       | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Possible error with nco "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_times=$(cdo ntime $filename)\n",
        "ncpdq -4 -O --cnk_plc=g3d --cnk_dmn=time,$n_times --cnk_dmn=lat,$n_lats --cnk_dmn=lon,$n_lons -a lon,lat,time $filename \"${output_dir}/step_5/ERA5_$(basename \"$filename\" .nc).nc\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Error in terminal:** *ncpdq: ERROR received 9 positional filenames; need exactly two*\n",
        "\n",
        "**How to fix:** \n",
        "Add after ncpdq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "echo \"n_times: $n_times\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "run script and check results. \n",
        "Correct value should display one line, eg.: *n_times: 365*\n",
        "\n",
        "If you see another line below value:\n",
        "\n",
        "> *n_times: 365* \n",
        "\n",
        "> *cdo ntime: Processed 1 variable [0.01s 44MB].*\n",
        "\n",
        "add to n_time definition code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_times=$(cdo ntime $filename  | awk 'NR==1 {print $1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. If you are getting an error in step 7 for `tas` variable not found in the data, follow the below steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. First verify if the variable name is var167 by running the code snippet below in your terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ncdump -h input_file.nc #replace input file with the tas file that you want to rename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Use `ncrename` function from `cdo` library to rename the variable from `var167` to `tas`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ncrename -v old_varname,new_varname input_file.nc #replace input file with the tas file that you want to rename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Run the script now and it should work. If not, re-check using ncdump -h your_file.nc to verify if variable is correctly renamed to `tas`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Visualizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is recommended that you visualize the data after completion of each step to verify if the data is being correctly produced. The data can be visualized by use of **ArcGIS** or **QGIS** or by utilizing below Python script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "def plot_nc_variable(nc_file, var_name, time_idx=1024):\n",
        "    dataset = Dataset(nc_file, 'r')\n",
        "    if var_name not in dataset.variables:\n",
        "        print(f\"Variable '{var_name}' not found in {nc_file}.\")\n",
        "        return\n",
        "    var_data = dataset.variables[var_name]\n",
        "    data = var_data[:, :, time_idx].T  \n",
        "    lats = dataset.variables['lat'][:]\n",
        "    lons = dataset.variables['lon'][:]\n",
        "\n",
        "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "\n",
        "    # Plot the data\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.contourf(lon_grid, lat_grid, data, cmap='viridis')\n",
        "    plt.colorbar(label=f\"{var_name}\")\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.title(f\"{var_name} at time index {time_idx}\")\n",
        "    plt.show()\n",
        "    dataset.close()\n",
        "\n",
        "nc_file = '${output_dir}/ERA5_tasrange_ERA5_1990_2022_t.nc'\n",
        "var_name = 'tasrange'\n",
        "plot_nc_variable(nc_file, var_name, time_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Memory error in Step 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you encounter crashes in Python/Jupyter during Step 9 due to insufficient memory, ensure that your system has at least 32 GB of RAM. Should the issue persist, close all active Python/Jupyter instances or restart your computer and attempt running the script again.\n",
        "\n",
        "To mitigate memory constraints, consider dividing the script into smaller segments and running them individually, except for the temperature and vapor pressure/humidity sections, which must be executed together. However, for optimal performance and to prevent memory-related issues, it is recommended to run the script on a machine or external server with 64 GB or more of RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5. Troubleshooting in Step 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To diagnose issues in Step 10, several commands can help identify where the problem arises, particularly related to timestamps, incorrect variables, or inconsistencies in variable units.\n",
        "\n",
        "1. Retain Temporary Files: Remove the final command in the code that deletes temporary files for specific variables. These files are valuable for troubleshooting. For each variable, four temporary files will be generated with the following naming convention: varname_b, varname_c, varname_d.\n",
        "\n",
        "2. Use CDO Info: Apply the cdo info command to each temporary file to pinpoint where the issue occurs.\n",
        "\n",
        "3. Verify Previous Steps: Review the earlier steps to ensure all were completed correctly. If any steps were performed incorrectly, repeat them and then re-execute Step 10.\n",
        "\n",
        "The most likely issue may be related to timestamps or inconsistent units. These can be resolved manually in Step 10 by adjusting the minutes or ensuring the units are consistent with the EMO-1 data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nco",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
